{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamranr123/kamranr123.github.io/blob/master/fine_tune_multilingual_bert_on_emotions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4on5D3v5YVng",
        "outputId": "0083b7a2-0e65-45c1-ade0-bcfd644749b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers datasets accelerate"
      ],
      "metadata": {
        "id": "oYBBBhdaILmf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P /content https://github.com/nazaninsbr/Persian-Emotion-Detection/raw/refs/heads/main/dataset.csv"
      ],
      "metadata": {
        "id": "WHRf3iBLeW9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "\n",
        "# Disable W&B\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "ZljdrqNY0a2w"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/dataset.csv\")\n",
        "\n",
        "# Emotion label columns (vote counts 0-5)\n",
        "label_cols = [\"Anger\", \"Fear\", \"Happiness\", \"Hatred\", \"Sadness\", \"Wonder\"]\n",
        "\n",
        "# Binarize emotions: 1 if >=3 votes (majority), else 0\n",
        "df[label_cols] = df[label_cols].apply(lambda row: [1 if x >= 3 else 0 for x in row], axis=1, result_type='expand')\n",
        "\n",
        "# Add Neutral: 1 if no emotions have majority (sum of binarized emotions == 0)\n",
        "df[\"Neutral\"] = (df[label_cols].sum(axis=1) == 0).astype(int)\n",
        "\n",
        "# Final labels\n",
        "final_labels = label_cols + [\"Neutral\"]\n",
        "df[\"labels\"] = df[final_labels].apply(lambda row: [float(x) for x in row], axis=1)\n",
        "\n",
        "# Keep only text + labels\n",
        "df = df[[\"text\", \"labels\"]]\n",
        "\n",
        "# Train/validation split\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Hugging Face datasets\n",
        "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
        "val_ds = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
        "dataset = DatasetDict({\"train\": train_ds, \"validation\": val_ds})\n",
        "\n",
        "# Verify\n",
        "train_labels = np.array(dataset[\"train\"][\"labels\"])\n",
        "val_labels = np.array(dataset[\"validation\"][\"labels\"])\n",
        "print(\"Unique training label values:\", np.unique(train_labels))\n",
        "print(\"Unique validation label values:\", np.unique(val_labels))\n",
        "print(\"Training label frequencies:\", np.sum(train_labels, axis=0) / train_labels.shape[0])\n",
        "print(\"Validation label frequencies:\", np.sum(val_labels, axis=0) / val_labels.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQowqp_3j9Hf",
        "outputId": "815980c0-b0d5-4655-fe4d-9abc8a9c5092"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique training label values: [0. 1.]\n",
            "Unique validation label values: [0. 1.]\n",
            "Training label frequencies: [0.05570833 0.02308333 0.023625   0.042125   0.05891667 0.033625\n",
            " 0.796125  ]\n",
            "Validation label frequencies: [0.04916667 0.02266667 0.02083333 0.04083333 0.05933333 0.02983333\n",
            " 0.80483333]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "\n",
        "# Check current label values\n",
        "train_labels = np.array(dataset[\"train\"][\"labels\"])\n",
        "val_labels = np.array(dataset[\"validation\"][\"labels\"])\n",
        "print(\"Unique training label values (before):\", np.unique(train_labels))\n",
        "print(\"Unique validation label values (before):\", np.unique(val_labels))\n",
        "\n",
        "# Binarize labels with a threshold (e.g., 0.5)\n",
        "thresholds = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.05]  # Lower threshold for Label 6\n",
        "def binarize_labels(example):\n",
        "    labels = np.array(example[\"labels\"])\n",
        "    example[\"labels\"] = (labels > np.array(thresholds)).astype(int)\n",
        "    return example\n",
        "\n",
        "dataset[\"train\"] = dataset[\"train\"].map(binarize_labels)\n",
        "dataset[\"validation\"] = dataset[\"validation\"].map(binarize_labels)\n",
        "\n",
        "# Verify binarization\n",
        "train_labels = np.array(dataset[\"train\"][\"labels\"])\n",
        "val_labels = np.array(dataset[\"validation\"][\"labels\"])\n",
        "print(\"Unique training label values (after):\", np.unique(train_labels))\n",
        "print(\"Unique validation label values (after):\", np.unique(val_labels))\n",
        "print(\"Training label frequencies:\", np.sum(train_labels, axis=0) / train_labels.shape[0])\n",
        "print(\"Validation label frequencies:\", np.sum(val_labels, axis=0) / val_labels.shape[0])"
      ],
      "metadata": {
        "id": "iCx_iLOQEeAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "aug = naw.SynonymAug(aug_p=0.3)\n",
        "def augment_data(example):\n",
        "    example[\"text\"] = aug.augment(example[\"text\"])[0]\n",
        "    return example\n",
        "dataset[\"train\"] = dataset[\"train\"].map(augment_data)"
      ],
      "metadata": {
        "id": "dUwgN3XxHiQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print(dataset[\"train\"][i]['labels'])"
      ],
      "metadata": {
        "id": "8EzJekzg9gs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = \"google-bert/bert-base-multilingual-cased\"\n",
        "model_name = \"HooshvareLab/bert-fa-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
        "\n",
        "dataset = dataset.map(tokenize, batched=True)\n",
        "# dataset = dataset.remove_columns([\"text\"])\n",
        "\n",
        "small_dataset = small_dataset.map(tokenize, batched=True)\n",
        "# small_dataset = small_dataset.remove_columns([\"text\"])"
      ],
      "metadata": {
        "id": "suYQzQhK0p_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4Ok0W230zzQ",
        "outputId": "9aa2e7cf-1378-4bee-de6b-fe95af023e47"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/bert-fa-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score, hamming_loss\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    probs = torch.sigmoid(torch.tensor(logits)).numpy()\n",
        "    labels = labels.astype(int)\n",
        "\n",
        "    thresholds = np.arange(0.1, 0.6, 0.1)\n",
        "    best_preds = np.zeros_like(labels)\n",
        "    best_thresholds = np.ones(labels.shape[1]) * 0.5\n",
        "    for i in range(labels.shape[1]):\n",
        "        best_f1 = 0\n",
        "        for t in thresholds:\n",
        "            preds_i = (probs[:, i] > t).astype(int)\n",
        "            f1_i = f1_score(labels[:, i], preds_i, average='binary')\n",
        "            if f1_i > best_f1:\n",
        "                best_f1 = f1_i\n",
        "                best_thresholds[i] = t\n",
        "                best_preds[:, i] = preds_i\n",
        "\n",
        "    f1_macro = f1_score(labels, best_preds, average='macro')\n",
        "    acc = accuracy_score(labels, best_preds)\n",
        "    hamming = hamming_loss(labels, best_preds)\n",
        "    per_label_f1 = f1_score(labels, best_preds, average=None)\n",
        "\n",
        "    print(f\"Best thresholds: {best_thresholds}\")\n",
        "    print(f\"Positive predictions: {np.sum(best_preds, axis=0)}\")\n",
        "    print(f\"Positive true labels: {np.sum(labels, axis=0)}\")\n",
        "    print(f\"Average logits: {np.mean(logits, axis=0)}\")\n",
        "    print(f\"Average probabilities: {np.mean(probs, axis=0)}\")\n",
        "\n",
        "    metrics = {\"accuracy\": acc, \"f1_macro\": f1_macro, \"hamming_loss\": hamming}\n",
        "    for i, f1_score_label in enumerate(per_label_f1):\n",
        "        metrics[f\"f1_label_{i}\"] = f1_score_label\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "7wHbVpd102tH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree('/content/bert-persian-emotions')"
      ],
      "metadata": {
        "id": "QbuClHldrUIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, DataCollatorWithPadding\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/content/bert-persian-emotions\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=500,\n",
        "    max_grad_norm=1.0,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        "    optim=\"adamw_torch_fused\"\n",
        ")"
      ],
      "metadata": {
        "id": "2M9Y0vJD07rg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "import torch.nn as nn\n",
        "\n",
        "# Focal loss\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1.0, gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "    def forward(self, logits, targets):\n",
        "        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(logits, targets)\n",
        "        pt = torch.exp(-bce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\").float()\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = FocalLoss(alpha=1.0, gamma=2.0)\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics  # your F1/multi-label metrics\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LkJnJTUd1-XX",
        "outputId": "34507de9-16e9-4a9a-da46-fcf42ff4a386",
        "collapsed": true
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-891948793.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = WeightedTrainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3750/3750 59:05, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>Hamming Loss</th>\n",
              "      <th>F1 Label 0</th>\n",
              "      <th>F1 Label 1</th>\n",
              "      <th>F1 Label 2</th>\n",
              "      <th>F1 Label 3</th>\n",
              "      <th>F1 Label 4</th>\n",
              "      <th>F1 Label 5</th>\n",
              "      <th>F1 Label 6</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "      <th>Steps Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.036912</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066245</td>\n",
              "      <td>0.608095</td>\n",
              "      <td>0.069809</td>\n",
              "      <td>0.055336</td>\n",
              "      <td>0.117260</td>\n",
              "      <td>0.054790</td>\n",
              "      <td>0.081868</td>\n",
              "      <td>0.059633</td>\n",
              "      <td>0.025016</td>\n",
              "      <td>24.468200</td>\n",
              "      <td>245.216000</td>\n",
              "      <td>30.652000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.046700</td>\n",
              "      <td>0.036805</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.062781</td>\n",
              "      <td>0.672238</td>\n",
              "      <td>0.071137</td>\n",
              "      <td>0.047711</td>\n",
              "      <td>0.102896</td>\n",
              "      <td>0.054790</td>\n",
              "      <td>0.084752</td>\n",
              "      <td>0.048762</td>\n",
              "      <td>0.029418</td>\n",
              "      <td>24.493800</td>\n",
              "      <td>244.960000</td>\n",
              "      <td>30.620000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.035800</td>\n",
              "      <td>0.040745</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064588</td>\n",
              "      <td>0.542119</td>\n",
              "      <td>0.071152</td>\n",
              "      <td>0.045361</td>\n",
              "      <td>0.098894</td>\n",
              "      <td>0.054790</td>\n",
              "      <td>0.082157</td>\n",
              "      <td>0.057252</td>\n",
              "      <td>0.042510</td>\n",
              "      <td>24.521600</td>\n",
              "      <td>244.683000</td>\n",
              "      <td>30.585000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.025800</td>\n",
              "      <td>0.051347</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>0.063761</td>\n",
              "      <td>0.499548</td>\n",
              "      <td>0.070899</td>\n",
              "      <td>0.046288</td>\n",
              "      <td>0.080275</td>\n",
              "      <td>0.055012</td>\n",
              "      <td>0.078897</td>\n",
              "      <td>0.065957</td>\n",
              "      <td>0.048998</td>\n",
              "      <td>24.411000</td>\n",
              "      <td>245.790000</td>\n",
              "      <td>30.724000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.025800</td>\n",
              "      <td>0.071458</td>\n",
              "      <td>0.224667</td>\n",
              "      <td>0.059906</td>\n",
              "      <td>0.290167</td>\n",
              "      <td>0.073712</td>\n",
              "      <td>0.043988</td>\n",
              "      <td>0.085845</td>\n",
              "      <td>0.046470</td>\n",
              "      <td>0.075503</td>\n",
              "      <td>0.052009</td>\n",
              "      <td>0.041812</td>\n",
              "      <td>24.555000</td>\n",
              "      <td>244.349000</td>\n",
              "      <td>30.544000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.007500</td>\n",
              "      <td>0.090177</td>\n",
              "      <td>0.461167</td>\n",
              "      <td>0.053673</td>\n",
              "      <td>0.124119</td>\n",
              "      <td>0.054745</td>\n",
              "      <td>0.024540</td>\n",
              "      <td>0.075472</td>\n",
              "      <td>0.047425</td>\n",
              "      <td>0.057582</td>\n",
              "      <td>0.063319</td>\n",
              "      <td>0.052632</td>\n",
              "      <td>24.494400</td>\n",
              "      <td>244.954000</td>\n",
              "      <td>30.619000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>0.097748</td>\n",
              "      <td>0.509333</td>\n",
              "      <td>0.048081</td>\n",
              "      <td>0.100024</td>\n",
              "      <td>0.057217</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.064748</td>\n",
              "      <td>0.034343</td>\n",
              "      <td>0.053996</td>\n",
              "      <td>0.057530</td>\n",
              "      <td>0.046512</td>\n",
              "      <td>24.498800</td>\n",
              "      <td>244.910000</td>\n",
              "      <td>30.614000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.101760</td>\n",
              "      <td>0.593667</td>\n",
              "      <td>0.048105</td>\n",
              "      <td>0.074024</td>\n",
              "      <td>0.055090</td>\n",
              "      <td>0.018116</td>\n",
              "      <td>0.073333</td>\n",
              "      <td>0.035088</td>\n",
              "      <td>0.058480</td>\n",
              "      <td>0.050114</td>\n",
              "      <td>0.046512</td>\n",
              "      <td>24.462700</td>\n",
              "      <td>245.272000</td>\n",
              "      <td>30.659000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.103112</td>\n",
              "      <td>0.609667</td>\n",
              "      <td>0.048273</td>\n",
              "      <td>0.070405</td>\n",
              "      <td>0.043887</td>\n",
              "      <td>0.021505</td>\n",
              "      <td>0.073826</td>\n",
              "      <td>0.031447</td>\n",
              "      <td>0.062927</td>\n",
              "      <td>0.057260</td>\n",
              "      <td>0.047059</td>\n",
              "      <td>24.538400</td>\n",
              "      <td>244.515000</td>\n",
              "      <td>30.564000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.103432</td>\n",
              "      <td>0.616833</td>\n",
              "      <td>0.049534</td>\n",
              "      <td>0.068643</td>\n",
              "      <td>0.046129</td>\n",
              "      <td>0.021008</td>\n",
              "      <td>0.075085</td>\n",
              "      <td>0.033898</td>\n",
              "      <td>0.064516</td>\n",
              "      <td>0.057613</td>\n",
              "      <td>0.048485</td>\n",
              "      <td>24.425000</td>\n",
              "      <td>245.650000</td>\n",
              "      <td>30.706000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best thresholds: [0.1 0.2 0.2 0.1 0.2 0.2 0.1]\n",
            "Positive predictions: [6000  120 1643 6000 5998  293 6000]\n",
            "Positive true labels: [217 133 182 169 256 143  76]\n",
            "Average logits: [-1.2192184 -1.5179837 -1.5103736 -1.556147  -1.224291  -1.5507237\n",
            " -1.8488623]\n",
            "Average probabilities: [0.22816226 0.17989072 0.18261063 0.1742728  0.22730808 0.17541349\n",
            " 0.13685417]\n",
            "Best thresholds: [0.2 0.2 0.2 0.1 0.2 0.2 0.1]\n",
            "Positive predictions: [4197 1418 2442 6000 4794 5148 4887]\n",
            "Positive true labels: [217 133 182 169 256 143  76]\n",
            "Average logits: [-1.3471718 -1.444151  -1.4945117 -1.4324857 -1.2597761 -1.169972\n",
            " -1.8958632]\n",
            "Average probabilities: [0.20664717 0.19142185 0.18831274 0.19293803 0.2219704  0.23851627\n",
            " 0.13385218]\n",
            "Best thresholds: [0.2 0.2 0.2 0.1 0.1 0.2 0.1]\n",
            "Positive predictions: [5208  837 1355 6000 5976 1953 1900]\n",
            "Positive true labels: [217 133 182 169 256 143  76]\n",
            "Average logits: [-1.1072237 -1.6361347 -1.8469177 -1.1143913 -1.5128623 -1.5163032\n",
            " -2.4054813]\n",
            "Average probabilities: [0.25126877 0.16545466 0.1503714  0.24945821 0.18538849 0.18437524\n",
            " 0.09086394]\n",
            "Best thresholds: [0.1 0.1 0.2 0.1 0.1 0.2 0.1]\n",
            "Positive predictions: [5453 3194  690 5866 3952  797 1271]\n",
            "Positive true labels: [217 133 182 169 256 143  76]\n",
            "Average logits: [-1.5320574 -2.1132002 -2.457679  -1.4453638 -1.8724393 -2.0964947\n",
            " -2.6366277]\n",
            "Average probabilities: [0.18984993 0.11810943 0.10232221 0.19934829 0.15177044 0.12309223\n",
            " 0.07778808]\n",
            "Best thresholds: [0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            "Positive predictions: [2035 2595  913 2069 2128 1549  498]\n",
            "Positive true labels: [217 133 182 169 256 143  76]\n",
            "Average logits: [-2.4479043 -2.2437952 -2.9306934 -2.435555  -2.4576004 -2.6798673\n",
            " -3.1627848]\n",
            "Average probabilities: [0.10395315 0.12799372 0.07050625 0.10040068 0.11241362 0.08919849\n",
            " 0.04975929]\n",
            "Best thresholds: [0.2 0.2 0.1 0.1 0.1 0.1 0.1]\n",
            "Positive predictions: [ 331  193  613 1307  786  773  342]\n",
            "Positive true labels: [217 133 182 169 256 143  76]\n",
            "Average logits: [-3.1915114 -3.3772116 -3.172181  -2.8602393 -3.2631135 -3.3216767\n",
            " -3.2761674]\n",
            "Average probabilities: [0.06195343 0.04926387 0.06076504 0.08573897 0.05917211 0.058568\n",
            " 0.04468657]\n",
            "Best thresholds: [0.1 0.1 0.3 0.1 0.1 0.1 0.2]\n",
            "Positive predictions: [552 587  96 821 670 448  53]\n",
            "Positive true labels: [217 133 182 169 256 143  76]\n",
            "Average logits: [-3.5574677 -3.4117384 -3.5198808 -3.2333808 -3.4600544 -3.6045368\n",
            " -3.587399 ]\n",
            "Average probabilities: [0.04799105 0.05000754 0.03953692 0.06151127 0.0556055  0.0418584\n",
            " 0.03231538]\n",
            "Best thresholds: [0.1 0.1 0.3 0.4 0.1 0.1 0.1]\n",
            "Positive predictions: [618 419 118 116 428 296  96]\n",
            "Positive true labels: [217 133 182 169 256 143  76]\n",
            "Average logits: [-3.49346   -3.5858064 -3.5326507 -3.3918724 -3.745113  -3.8222709\n",
            " -3.7526786]\n",
            "Average probabilities: [0.0529051  0.04154375 0.04116195 0.05491509 0.04137359 0.03242992\n",
            " 0.02682189]\n",
            "Best thresholds: [0.1 0.1 0.3 0.3 0.1 0.1 0.1]\n",
            "Positive predictions: [421 332 116 149 475 346  94]\n",
            "Positive true labels: [217 133 182 169 256 143  76]\n",
            "Average logits: [-3.7001836 -3.6950746 -3.5753233 -3.5086913 -3.6914055 -3.7733974\n",
            " -3.7820466]\n",
            "Average probabilities: [0.04225694 0.03684922 0.0398326  0.04842798 0.04481622 0.03509273\n",
            " 0.02594669]\n",
            "Best thresholds: [0.1 0.1 0.3 0.3 0.1 0.1 0.1]\n",
            "Positive predictions: [390 343 111 126 457 343  89]\n",
            "Positive true labels: [217 133 182 169 256 143  76]\n",
            "Average logits: [-3.747147  -3.6812637 -3.585737  -3.563783  -3.7099786 -3.7732959\n",
            " -3.7944095]\n",
            "Average probabilities: [0.03978473 0.03739673 0.0391341  0.04518981 0.04377173 0.03519052\n",
            " 0.02548344]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3750, training_loss=0.015812524233261745, metrics={'train_runtime': 3547.0663, 'train_samples_per_second': 67.662, 'train_steps_per_second': 1.057, 'total_flos': 3.1574744064e+16, 'train_loss': 0.015812524233261745, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique label values:\", np.unique(small_dataset[\"validation\"][\"labels\"]))"
      ],
      "metadata": {
        "id": "l_47YpJ1-ZNZ",
        "outputId": "e7994b24-a568-4a0b-e1f1-fa0a28313df1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique label values: [0.         0.07692308 0.08333333 0.08333333 0.09090909 0.1\n",
            " 0.1        0.1        0.11111111 0.11111111 0.11111111 0.125\n",
            " 0.125      0.14285714 0.14285714 0.15384615 0.16666667 0.16666667\n",
            " 0.18181818 0.2        0.2        0.2        0.22222222 0.22222222\n",
            " 0.22222222 0.23076923 0.25       0.25       0.27272727 0.28571429\n",
            " 0.28571429 0.3        0.3        0.3        0.30769231 0.33333333\n",
            " 0.33333333 0.33333333 0.36363636 0.375      0.375      0.4\n",
            " 0.42857143 0.42857143 0.44444444 0.44444444 0.44444444 0.5\n",
            " 0.5        0.57142857 0.57142857 0.6        0.66666667 0.75\n",
            " 1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"من امروز خیلی خوشحالم\",      # Happy\n",
        "    \"احساس می‌کنم ناراحت و خسته‌ام\", # Sad\n",
        "    \"از تاریکی می‌ترسم\",           # Fear\n",
        "    \"قدم زدن زیر بارون شاید بهترین مسکن درد هاست...\" # Neutral\n",
        "]\n",
        "\n",
        "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "outputs = model(**inputs)\n",
        "probs = torch.sigmoid(outputs.logits).detach().numpy()\n",
        "\n",
        "for text, p in zip(texts, probs):\n",
        "    labels_pred = [final_labels[i] for i, v in enumerate(p) if v > 0.5]\n",
        "    print(text, \"->\", labels_pred)\n"
      ],
      "metadata": {
        "id": "rtoVASNe-pvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "# collect true labels and preds on validation set\n",
        "preds_logits = trainer.predict(dataset[\"validation\"]).predictions  # raw logits\n",
        "probs = 1 / (1 + np.exp(-preds_logits))\n",
        "preds = (probs > 0.5).astype(int)\n",
        "\n",
        "# binarize references (soft labels → 0/1)\n",
        "refs = np.stack([ex[\"labels\"] for ex in dataset[\"validation\"]])\n",
        "refs_bin = (refs >= 0.5).astype(int)\n",
        "\n",
        "# overall per-label counts\n",
        "pos_counts = refs_bin.sum(axis=0)\n",
        "neg_counts = refs_bin.shape[0] - pos_counts\n",
        "print(\"pos counts per label:\", pos_counts)\n",
        "print(\"neg counts per label:\", neg_counts)\n",
        "\n",
        "# per-label f1\n",
        "for i, name in enumerate(final_labels):\n",
        "    print(name, \"F1:\", f1_score(refs_bin[:, i], preds[:, i], zero_division=0))\n",
        "\n",
        "# full classification report\n",
        "print(classification_report(refs_bin, preds, zero_division=0))\n"
      ],
      "metadata": {
        "id": "AshUjyI7FgQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "# macro F1:\n",
        "macro = f1_score(refs, preds, average=\"macro\", zero_division=0)\n",
        "micro = f1_score(refs, preds, average=\"micro\", zero_division=0)\n",
        "print(\"macro, micro:\", macro, micro)\n"
      ],
      "metadata": {
        "id": "xjBI7MErFY6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert Hugging Face dataset into numpy array of labels\n",
        "all_labels = np.stack(dataset[\"train\"][\"labels\"])  # shape: (num_samples, num_labels)\n",
        "\n",
        "# Count positives and negatives per label\n",
        "pos_counts = all_labels.sum(axis=0)\n",
        "neg_counts = all_labels.shape[0] - pos_counts\n",
        "\n",
        "print(\"pos counts per label:\", pos_counts)\n",
        "print(\"neg counts per label:\", neg_counts)\n",
        "\n",
        "# If you want mapping to label names:\n",
        "for name, pos, neg in zip(final_labels, pos_counts, neg_counts):\n",
        "    print(f\"{name:10s} | pos: {int(pos):5d} | neg: {int(neg):5d}\")\n",
        "\n",
        "val_labels = np.array(small_dataset[\"validation\"][\"labels\"])\n",
        "print(\"Validation label frequencies:\", np.sum(val_labels, axis=0) / val_labels.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U55f3uwAXOki",
        "outputId": "46a0d526-24ab-451d-b559-e46900464545"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos counts per label: [4435.79776857 3377.90284005 3411.00137803 4083.14986549 4608.76150663\n",
            " 3826.38664122  257.        ]\n",
            "neg counts per label: [19564.20223143 20622.09715995 20588.99862197 19916.85013451\n",
            " 19391.23849337 20173.61335878 23743.        ]\n",
            "Anger      | pos:  4435 | neg: 19564\n",
            "Fear       | pos:  3377 | neg: 20622\n",
            "Happiness  | pos:  3411 | neg: 20588\n",
            "Hatred     | pos:  4083 | neg: 19916\n",
            "Sadness    | pos:  4608 | neg: 19391\n",
            "Wonder     | pos:  3826 | neg: 20173\n",
            "Neutral    | pos:   257 | neg: 23743\n",
            "Validation label frequencies: [0.16867392 0.14521033 0.14398886 0.1701612  0.18877742 0.16365703\n",
            " 0.01953125]\n"
          ]
        }
      ]
    }
  ]
}