{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamranr123/kamranr123.github.io/blob/master/km_ui_sd%20v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> KM Colab</h1>"
      ],
      "metadata": {
        "id": "Ww9RtC1NhlgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "Flux_mode = False # @param {\"type\":\"boolean\",\"placeholder\":\"False\"}\n",
        "\n",
        "def gn():\n",
        "\n",
        "    return 'TotoroUI' if Flux_mode else 'CKMyUI'.replace(\"KM\", 'omf')\n",
        "\n",
        "gnn= 'TotoroUI' if Flux_mode else 'KMUI'\n",
        "\n",
        "def check_mode():\n",
        "    if Flux_mode:\n",
        "        raise Exception(\"Wrone Mode !, it is Flux mode.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "D-0_qj5YQbt7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## initial"
      ],
      "metadata": {
        "id": "b-lSsUXClM0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "\n",
        "import wget\n",
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "\n",
        "# ******************************************************************************\n",
        "class Modelpaths:\n",
        "    base_path = f'/content/{gnn}/models'\n",
        "    model = f'{base_path}/checkpoints'\n",
        "    lora = f'{base_path}/loras'\n",
        "    vae = f'{base_path}/vae'\n",
        "    upscale = f'{base_path}/upscale_models'\n",
        "    controlnet = f'{base_path}/controlnet'\n",
        "    embeddings = f'{base_path}/embeddings'\n",
        "    diffusers = f'{base_path}/diffusers'\n",
        "    unet = f'{base_path}/unet'\n",
        "    clip = f'{base_path}/clip'\n",
        "\n",
        "    def __init__(self):\n",
        "        if not os.path.exists(self.base_path):\n",
        "            os.makedirs(self.model)\n",
        "            os.makedirs(self.lora)\n",
        "            os.makedirs(self.vae)\n",
        "            os.makedirs(self.upscale)\n",
        "            os.makedirs(self.embeddings)\n",
        "            os.makedirs(self.diffusers)\n",
        "            os.makedirs(self.unet)\n",
        "            os.makedirs(self.clip)\n",
        "\n",
        "modelpaths = Modelpaths()\n",
        "\n",
        "# ******************************************************************************\n",
        "if Flux_mode:\n",
        "    %cd /content\n",
        "    !git clone -b totoro3 https://github.com/camenduru/ComfyUI /content/TotoroUI\n",
        "%cd /content/{gn()}\n",
        "\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.27.post2\n",
        "!pip install spandrel\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "# ******************************************************************************\n",
        "def download(model_link, model_name, path=modelpaths.model):\n",
        "    if 'civitai' in model_link:\n",
        "        if \"?\" in model_link:\n",
        "            model_link = f\"{model_link},token=2a98e142e24406e7fbb077e80b0418a6\"\n",
        "        else:\n",
        "            model_link = f\"{model_link}?token=2a98e142e24406e7fbb077e80b0418a6\"\n",
        "        # file_name = path + '/' + model_name # get the full path to the file\n",
        "        # if os.path.exists(file_name):\n",
        "        #     os.remove(file_name) # if exists, remove it directly\n",
        "        # file_name = wget.download(model_link, out=file_name)\n",
        "        # print(file_name)\n",
        "\n",
        "        !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {model_link} --dir={path} --out={model_name}\n",
        "    else:\n",
        "        if path == modelpaths.model:\n",
        "            !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {model_link} --dir={path} --out={model_name}\n",
        "        else:\n",
        "            !aria2c --console-log-level=error -c -x 16 -s 8 -k 1M {model_link} --dir={path} --out={model_name}\n",
        "\n",
        "def replace_word_in_file(file_path, target_word, new_word):\n",
        "    try:\n",
        "        # Open the file in read mode\n",
        "        with open(file_path, 'r') as file:\n",
        "            # Read the file content\n",
        "            file_content = file.read()\n",
        "\n",
        "        # Replace the target word with the new word\n",
        "        modified_content = file_content.replace(target_word, new_word)\n",
        "        modified_content = modified_content.replace(f'{gnn}-Impact-Subpack', 'ComfyUI-Impact-Subpack') #exeption\n",
        "\n",
        "        # Open the file in write mode to overwrite its content\n",
        "        with open(file_path, 'w') as file:\n",
        "            # Write the modified content back to the file\n",
        "            file.write(modified_content)\n",
        "\n",
        "        # print(f\"Word '{target_word}' replaced with '{new_word}' in {file_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}: {file_path}\")\n",
        "\n",
        "def forceCopyFile (sfile, dfile):\n",
        "    if os.path.isfile(sfile):\n",
        "        shutil.copy2(sfile, dfile)\n",
        "\n",
        "def forceMoveFile (sfile, dfile):\n",
        "    if os.path.isfile(sfile):\n",
        "        shutil.move(sfile, dfile)\n",
        "\n",
        "def isAFlatDir(sDir):\n",
        "    for item in os.listdir(sDir):\n",
        "        sItem = os.path.join(sDir, item)\n",
        "        if os.path.isdir(sItem):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def moveTree(src, dst, target_word='Comfy', new_word=gnn):\n",
        "    _dst = dst.replace(target_word, new_word)\n",
        "    _dst = _dst.replace(target_word.lower(), new_word.lower())\n",
        "\n",
        "    for item in os.listdir(src):\n",
        "        _item = item.replace(target_word, new_word)\n",
        "        _item = _item.replace(target_word.lower(), new_word.lower())\n",
        "        s = os.path.join(src, item)\n",
        "        d = os.path.join(_dst, _item)\n",
        "        if 'comfy_types' in item:\n",
        "            print(\"111  ************* ------------------------------------------------\")\n",
        "            print(s, d, item)\n",
        "\n",
        "        if os.path.isfile(s):\n",
        "            if not os.path.exists(_dst):\n",
        "                os.makedirs(_dst)\n",
        "            forceMoveFile(s,d)\n",
        "            replace_word_in_file(d, target_word, new_word)\n",
        "            replace_word_in_file(d, target_word.lower(), new_word.lower())\n",
        "        if os.path.isdir(s):\n",
        "            isRecursive = not isAFlatDir(s)\n",
        "            if isRecursive:\n",
        "                moveTree(s, d)\n",
        "            else:\n",
        "                if not os.path.exists(d):\n",
        "                    os.makedirs(d)\n",
        "                for item2 in os.listdir(s):\n",
        "                    _item = item2.replace(target_word, new_word)\n",
        "                    _item = _item.replace(target_word.lower(), new_word.lower())\n",
        "                    srcFile = os.path.join(s, item2)\n",
        "                    dstFile = os.path.join(d, _item)\n",
        "                    forceMoveFile(srcFile, dstFile)\n",
        "                    replace_word_in_file(dstFile, target_word, new_word)\n",
        "                    replace_word_in_file(dstFile, target_word.lower(), new_word.lower())\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "InBOxoWDlRhK"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download model (only for non Flux)"
      ],
      "metadata": {
        "id": "oHhGb05cla8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://civitai.com/api/download/models/641087?type=Model&format=SafeTensor&size=full&fp=fp16              ZavyChromaXL.V9.safetensors\n",
        "\n",
        "https://civitai.com/api/download/models/671503?type=Model&format=SafeTensor&size=full&fp=fp16           RealCartoon-Realistic_v17.safetensors\n"
      ],
      "metadata": {
        "id": "TqhLkRNdBJPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown extensions (custom node)\n",
        "model_link = \"https://civitai.com/api/download/models/671503\" # @param {\"type\":\"string\",\"placeholder\":\"enter link of model to download\"}\n",
        "model_name = \"RealCartoon-Realistic_v17.safetensors\" # @param {\"type\":\"string\",\"placeholder\":\"enter name of model\"}\n",
        "model_type = \"Checkpoint\" # @param [\"Checkpoint\",\"LoRa\",\"ControlNet\",\"None\"] {\"type\":\"string\"}\n",
        "\n",
        "if model_type == \"LoRa\":\n",
        "    %cd {modelpaths.lora}\n",
        "    download(model_link, model_name, modelpaths.lora)\n",
        "elif model_type == \"Checkpoint\":\n",
        "    %cd {modelpaths.model}\n",
        "    download(model_link, model_name, modelpaths.model)\n",
        "elif model_type == \"ControlNet\":\n",
        "    %cd {modelpaths.controlnet}\n",
        "    download(model_link, model_name, modelpaths.controlnet)\n",
        "elif model_type == \"None\":\n",
        "    %cd /content/\n",
        "    download(model_link, model_name, '/content')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kZKaq7b3eKha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aab9468-15df-4fda-e218-cc3c7a4ed901"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KMUI/models/checkpoints\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "e423ab|\u001b[1;32mOK\u001b[0m  |   118MiB/s|/content/KMUI/models/checkpoints/RealCartoon-Realistic_v17.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUabsHbSTKF6"
      },
      "source": [
        "## LoRa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU43pGOkO8A3"
      },
      "outputs": [],
      "source": [
        "lora_list = []\n",
        "lora_list.append(['https://civitai.com/api/download/models/122580', 'Skin-Hands.safetensors']) # Skin & Hands (male/female) from Polyhedron\n",
        "lora_list.append(['https://civitai.com/api/download/models/117151', 'LEOSAMClothingAdjuster.safetensors']) # LEOSAM's Clothing +/- Adjuster LoRA\n",
        "lora_list.append(['https://civitai.com/api/download/models/126785','WowifierXL.safetensors']) # WowifierXL LoRA\n",
        "lora_list.append(['https://civitai.com/api/download/models/155625','Caricaturized-xl.safetensors']) # SDXL Caricaturized LoRA\n",
        "lora_list.append(['https://huggingface.co/naonovn/Lora/resolve/main/add_detail.safetensors','add_detail.safetensors']) # add_detail LoRA\n",
        "\n",
        "# 3D rendering style (SD 1.5)\n",
        "# https://civitai.com/models/73756\n",
        "# The larger the version number, the more mature and realistic the rendering style will be.\n",
        "lora_list.append(['https://civitai.com/api/download/models/107366','3DMM_V12.safetensors'])\n",
        "lora_list.append(['https://civitai.com/api/download/models/78467','3DMM_V10.safetensors'])\n",
        "lora_list.append(['https://civitai.com/api/download/models/88206','3DMM_V7.safetensors'])\n",
        "lora_list.append(['https://civitai.com/api/download/models/78559','3DMM_V5.safetensors'])\n",
        "lora_list.append(['https://civitai.com/api/download/models/78564','3DMM_V3.safetensors'])\n",
        "\n",
        "# Detail Tweaker XL\n",
        "# https://civitai.com/models/122359/detail-tweaker-xl\n",
        "# lora_list.append(['https://civitai.com/api/download/models/135867','DetailTweaker-XL-V1.safetensors'])\n",
        "\n",
        "# Add More Details - Detail Enhancer / Tweaker\n",
        "# https://civitai.com/models/82098/add-more-details-detail-enhancer-tweaker-lora\n",
        "lora_list.append(['https://civitai.com/api/download/models/87153','AddMoreDetails-v1.safetensors'])\n",
        "\n",
        "# sharpen/soften effect\n",
        "# https://civitai.com/models/94543/lora-sharpensoften-effect-lora-model\n",
        "# lora_list.append(['https://civitai.com/api/download/models/100851?type=Model&format=SafeTensor','sharpen-soften effect-v1.safetensors'])\n",
        "\n",
        "# S-shape body slider LoRA (SD 1.5)\n",
        "# https://civitai.com/models/135052/muggle-loras-shape-body-slider\n",
        "# lora_list.append(['https://civitai.com/api/download/models/148789?type=Model&format=SafeTensor','S-shape body slider-v1.safetensors'])\n",
        "\n",
        "# Better eyes+face+skin LoRA (SD 1.5)\n",
        "# https://civitai.com/models/51430?modelVersionId=55905\n",
        "# lora_list.append(['https://civitai.com/api/download/models/55905','BetterEyesFaceSkin-v1.safetensors'])\n",
        "\n",
        "# Hipoly 3D Model LoRA (SD 1.5)\n",
        "# https://civitai.com/models/70921/duchaitenniji\n",
        "# lora_list.append(['https://civitai.com/api/download/models/44566','Hipoly3D-v2.safetensors'])\n",
        "\n",
        "# Samaritan 3d Cartoon SDXL\n",
        "# https://civitai.com/models/121932/samaritan-3d-cartoon-sdxl\n",
        "# the default face is grumpy/angry for some reason. But this model was trained on variety of emotions,\n",
        "# try \"smiling, laugh,sad, crying, shouting, surprised, etc\" in the prompt\n",
        "# lora_list.append(['https://civitai.com/api/download/models/132727','Samaritan-3d-Cartoon-xl.safetensors'])\n",
        "\n",
        "# xl-water-dress\n",
        "# https://civitai.com/models/156447/xl-water-dress\n",
        "# lora_list.append(['https://civitai.com/api/download/models/175608','xl-water-dress.safetensors'])\n",
        "\n",
        "# xl_more_art-full\n",
        "# https://civitai.com/models/124347/xlmoreart-full-xlreal-enhancer?modelVersionId=152309\n",
        "# lora_list.append(['https://civitai.com/api/download/models/152309','xl_more_art-full-v1.safetensors'])\n",
        "\n",
        "# cowgirl with hands on knees\n",
        "# lora_list.append(['https://civitai.com/api/download/models/140297?type=Model&format=SafeTensor','cowgirl_with_hands_on_knees_v1.0.safetensors'])\n",
        "\n",
        "\n",
        "# POV Squatting Cowgirl LoRA\n",
        "# lora_list.append(['https://civitai.com/api/download/models/10490?type=Model&format=SafeTensor&size=full&fp=fp16','PSCowgirl.safetensors'])\n",
        "\n",
        "# colorfulhair2 LoRA\n",
        "# lora_list.append(['https://civitai.com/api/download/models/97974?type=Model&format=SafeTensor', 'asb-CH2.safetensors'])\n",
        "\n",
        "# Half Color Hair LoRA\n",
        "# lora_list.append(['https://civitai.com/api/download/models/45686','hlfcol.safetensors'])\n",
        "\n",
        "# color hair LoRA\n",
        "# lora_list.append(['https://civitai.com/api/download/models/113573?type=Model&format=SafeTensor','color-hair.safetensors'])\n",
        "\n",
        "\n",
        "# lora_list.extend(extract_lora_from_author(author='casque'))\n",
        "# lora_list.extend(extract_lora_from_rep(repo_id='naonovn/Lora'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qf_1-GCqcVt"
      },
      "source": [
        "# Run KMUI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setting\n",
        "\n",
        "#@markdown # UI\n",
        "#@markdown extensions (custom node)\n",
        "ReactorNode = True #@param {type:'boolean'}\n",
        "ControlnetAux = False #@param {type:'boolean'}\n",
        "#@markdown download\n",
        "DownloadEmbeddings = False #@param {type:'boolean'}\n",
        "DownloadLoRa = False #@param {type:'boolean'}\n",
        "DownloadVAE = False #@param {type:'boolean'}\n",
        "Clip_Vision_g = False #@param {type:'boolean'}"
      ],
      "metadata": {
        "id": "IzWiMDvTKPqO",
        "cellView": "form"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "g_dX9-EbDiXA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "check_mode()\n",
        "\n",
        "#@title Download models\n",
        "if DownloadEmbeddings:\n",
        "    !wget -q 'https://huggingface.co/nolanaatama/colab/resolve/main/embeddings.zip' -P /content/{gn()}/models/embeddings/\n",
        "    with zipfile.ZipFile(f\"/content/{gn()}/models/embeddings/embeddings.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(f'/content/{gn()}/models')\n",
        "    os.remove(f\"/content/{gn()}/models/embeddings/embeddings.zip\")\n",
        "\n",
        "if DownloadLoRa:\n",
        "    %cd {lora_path}\n",
        "    for item in lora_list:\n",
        "      download(item[0], item[1], modelpaths.lora)\n",
        "\n",
        "if DownloadVAE:\n",
        "    download('https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt', 'vae-ft-mse-840000-ema-pruned.ckpt', modelpaths.vae)\n",
        "\n",
        "# if ReactorNode:\n",
        "#     download(\"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth\", 'GFPGANv1.4.pth', f'{modelpaths.base_path}/facerestore_models')\n",
        "#     download(\"https://huggingface.co/ezioruan/inswapper_128.onnx/resolve/main/inswapper_128.onnx\", 'inswapper_128.onnx', f'{modelpaths.base_path}/insightface')\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "rld0qAZAfPg0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Prepare workflow\n",
        "\n",
        "check_mode()\n",
        "\n",
        "%cd /content\n",
        "!apt -y update -qq\n",
        "!wget https://github.com/camenduru/gperftools/releases/download/v1.0/libtcmalloc_minimal.so.4 -O /content/libtcmalloc_minimal.so.4\n",
        "%env LD_PRELOAD=/content/libtcmalloc_minimal.so.4\n",
        "\n",
        "# !pip install -q mediapipe==0.9.1.0 addict yapf fvcore omegaconf\n",
        "\n",
        "!git clone https://github.com/comfyanonymous/{gn()}\n",
        "\n",
        "\n",
        "%cd /content/{gn()}/custom_nodes\n",
        "if ControlnetAux:\n",
        "    !git clone https://github.com/Fannovel16{gn()}_controlnet_aux/\n",
        "\n",
        "if ReactorNode:\n",
        "    !git clone https://github.com/Gourieff/{gn()}-reactor-node {gn()[:-2]}_reactor_node\n",
        "\n",
        "moveTree(f'/content/{gn()}', f'/content/{gnn}')\n",
        "shutil.rmtree(f'/content/{gn()}')\n",
        "\n",
        "# install requirements\n",
        "%cd /content/{gnn}\n",
        "# C_omfy\n",
        "!pip install xformers==0.0.2 -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "\n",
        "%cd /content/{gnn}/custom_nodes\n",
        "\n",
        "# reactor-node\n",
        "if ReactorNode:\n",
        "    !pip install -r {gnn}_reactor_node/requirements.txt\n",
        "    !python {gnn}_reactor_node/install.py\n",
        "\n",
        "\n",
        "# controlnet_aux\n",
        "if ControlnetAux:\n",
        "    !pip install -r {gnn}_controlnet_aux/requirements.txt\n",
        "\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb8G2WauywJu"
      },
      "source": [
        "# Utiities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-6yNrqCkO0i"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "# shutil.move('/content/models', '/content/KMUI/models')\n",
        "# shutil.move('/content/KMUI/models', '/content/models')\n",
        "# shutil.move('/content/stable-diffusion-webui/models/Stable-diffusion', '/content/KMUI/models/checkpoints')\n",
        "shutil.rmtree('/content/KMUI')\n",
        "# shutil.rmtree('/content/ComfyUI')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jfgnf3GbPcoK"
      },
      "outputs": [],
      "source": [
        "#@title Saving images\n",
        "\n",
        "#@markdown <small>The zip file will be visible at the files tab.</small>\n",
        "from datetime import datetime\n",
        "str_date = datetime.today().strftime('%Y-%m-%d-%H%M%S')\n",
        "archive_name = f\"outputs-{str_date}.zip\"\n",
        "\n",
        "print(\"Zipping...\")\n",
        "!zip -qr /content/{archive_name} /content/KMUI/output\n",
        "print(f\"\\033[92mZipped. You can now find {archive_name} at the files tab.\\033[0m\")\n",
        "\n",
        "# ----\n",
        "\n",
        "#@markdown <small>This copies the zip file to your Google Drive</small>\n",
        "copy_to_gdrive = True #@param {type:'boolean'}\n",
        "gdrive_folder = \"AI/Generated\" #@param { 'type': 'string' }\n",
        "\n",
        "if copy_to_gdrive:\n",
        "  # utility.log_usage('zip-to-gdrive')\n",
        "  from google.colab import drive\n",
        "\n",
        "  print(\"Mounting to Google Drive...\")\n",
        "  drive.mount('/content/drive')\n",
        "  if gdrive_folder == \"\":\n",
        "    gdrive_folder = \"AI/Generated\"\n",
        "\n",
        "  drive_folder = f\"/content/drive/MyDrive/{gdrive_folder}\"\n",
        "\n",
        "  !mkdir -p {drive_folder}\n",
        "  !cp /content/{archive_name} {drive_folder}\n",
        "  print(f\"\\033[92mCopied to {gdrive_folder}!\\033[0m\")\n",
        "\n",
        "  drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ai Model for generate prompt\n",
        "Meta-Llama-3-8B-Instruct-abliterated-v3-GGUF"
      ],
      "metadata": {
        "id": "u6v6g3pXAjMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi\n",
        "!pip install huggingface_hub\n",
        "!pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "# !pip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "\n",
        "# from huggingface_hub import login\n",
        "# login(token='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')  # Replace with your actual token\n",
        "\n",
        "download('https://huggingface.co/spaces/kamran-r123/SD-Prompt-Generator/resolve/main/prompt_style.txt?download=true', 'prompt_style.txt', '/content')\n",
        "promptGenerator = None\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "Hnx8jVY6ChsK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "import re\n",
        "import json\n",
        "\n",
        "class PromptGenerator:\n",
        "    chat_history = []\n",
        "\n",
        "    class Item:\n",
        "        prompt: str\n",
        "        temperature: float = 0.6\n",
        "        max_new_tokens: int = 1024\n",
        "        seed : int = 43\n",
        "\n",
        "    def __init__(self):\n",
        "        self.system_prompt = self.read_file('/content/prompt_style.txt')\n",
        "        self.len_chat_history = 0\n",
        "        self.chat_history = []\n",
        "        model_id = \"failspy/Meta-Llama-3-8B-Instruct-abliterated-v3-GGUF\"\n",
        "        self.model = Llama.from_pretrained(repo_id=model_id, filename=\"*-v3_q6.gguf\", n_gpu_layers=-1, n_ctx=4096, verbose=False)\n",
        "\n",
        "    def json_extractor_from_text(self, text):\n",
        "        text = text.replace(\" '\", ' \"').replace(\"' \", '\" ').replace(\"{'\", '{\"').replace(\"'}\", '\"}').replace(\"':\", '\":')\n",
        "        text = text.replace(\"',\", '\",')\n",
        "        start_index = text.find('{')\n",
        "        end_index = text.rfind('}')\n",
        "        if end_index == -1:  # If no closing '}' is found\n",
        "            text += '\"}'  # Add missing closing brace\n",
        "            end_index = len(text)   # Set end_index to the new last character\n",
        "\n",
        "        # Step 3: Extract the JSON part from the start index to the end index\n",
        "        json_string = text[start_index:end_index + 1]\n",
        "        json_string = json_string.replace('no ', '').replace('No ', '')\n",
        "        json_string = json_string.replace('\\n', '').replace('\\r', '').strip()\n",
        "\n",
        "        try:\n",
        "            # Parse the JSON string\n",
        "            return json.loads(json_string)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"json_extractor_from_text: {e}\")\n",
        "            return None\n",
        "\n",
        "    def read_file(self, file_path):\n",
        "        with open(file_path, 'r') as file:\n",
        "            contents = file.read()\n",
        "        return contents\n",
        "\n",
        "\n",
        "    def truncate_list_and_append(self, new_string):\n",
        "        \"\"\"Truncate strings in the list such that their total length does not exceed 4096 characters,\n",
        "        and append `new_string` to the list while removing the first element if necessary.\n",
        "        \"\"\"\n",
        "        ln = len(''.join(new_string))\n",
        "        max_sum = 1024 * 8\n",
        "        if len(self.chat_history) == 0:\n",
        "            self.len_chat_history = ln\n",
        "            self.chat_history.append(new_string)\n",
        "            return\n",
        "        if ln > max_sum:\n",
        "            self.len_chat_history = ln\n",
        "            self.chat_history.clear()\n",
        "            self.chat_history.append(new_string)\n",
        "            return\n",
        "\n",
        "        while len(self.chat_history) > 0 and ln + self.len_chat_history > max_sum:\n",
        "            self.len_chat_history -= len(''.join(self.chat_history.pop(0)))\n",
        "        self.chat_history.append(new_string)\n",
        "\n",
        "    def free_memory(self):\n",
        "        self.model.reset()\n",
        "        self.model.set_cache(None)\n",
        "        del self.model\n",
        "        self.model = None\n",
        "\n",
        "    def format_prompt(self, item: Item):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "        ]\n",
        "        for it in self.chat_history:\n",
        "            messages.append({\"role\" : \"user\", \"content\": it[0]})\n",
        "            messages.append({\"role\" : \"assistant\", \"content\": it[1]})\n",
        "        messages.append({\"role\" : \"user\", \"content\": item.prompt})\n",
        "        return messages\n",
        "\n",
        "    def generate_prompt(self, prompt, seed=4):\n",
        "        item = PromptGenerator.Item()\n",
        "        item.seed=seed\n",
        "        item.prompt = prompt\n",
        "\n",
        "        formatted_prompt = self.format_prompt(item)\n",
        "        output = self.model.create_chat_completion(messages=formatted_prompt, seed=item.seed, temperature=item.temperature,\n",
        "                                              max_tokens=item.max_new_tokens)\n",
        "\n",
        "\n",
        "        out = output['choices'][0]['message']['content']\n",
        "        answer = self.json_extractor_from_text(str(out))\n",
        "        if not answer:\n",
        "            answer = out.replace('no ', '').replace('No ', '')\n",
        "\n",
        "        self.truncate_list_and_append([str(item.prompt), str(answer)])\n",
        "        return answer, output\n",
        "\n",
        "    def clear_history(self):\n",
        "        self.chat_history.clear()\n",
        "\n",
        "if promptGenerator:\n",
        "    promptGenerator.free_memory()\n",
        "promptGenerator = PromptGenerator()"
      ],
      "metadata": {
        "id": "_oSVT4RRAtxC"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g, _ = promptGenerator.generate_prompt(prompt = 'create image prompt about girl stay in hotel, she is lying on her bed', seed=4)\n",
        "promptGenerator.clear_history()\n",
        "print(g)"
      ],
      "metadata": {
        "id": "0dbNNsgKD0v7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(g)"
      ],
      "metadata": {
        "id": "Ci_6uDLO8rA7",
        "outputId": "bd25e3d1-4fd6-44f6-a2df-fa45b0647e31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'positive': \"A young girl, 18-22 years old, with long, curly brown hair and bright blue eyes, lies on her bed in a luxurious hotel room. The bed is adorned with soft, white sheets and a plush, cream-colored comforter. The girl is wearing a white, lacy nightgown and has a relaxed, peaceful expression on her face. The room is dimly lit, with a soft, warm glow emanating from a table lamp in the corner. The walls are adorned with elegant, golden trim and the floor is covered with a plush, cream-colored carpet. The girl's hair is loose and flowing down her back, and her eyes are closed, as if she is sleeping. The overall atmosphere is serene and peaceful, evoking a sense of relaxation and tranquility.\", 'negative': 'text, humans, man-made structures, artificial lighting, technology, urban characteristics, aquatic features, floral elements, vehicles, animal figures, signs or advertisements'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nodes"
      ],
      "metadata": {
        "id": "imVQcYXHvVeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models as Enum"
      ],
      "metadata": {
        "id": "p50s1XPcng-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "\n",
        "# https://huggingface.co/stabilityai/control-lora\n",
        "class ControlnetLoRa_SDXL(Enum):\n",
        "    Canny = ['https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-canny-rank256.safetensors', 'control-lora-canny-rank256.safetensors']\n",
        "    Depth = ['https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-depth-rank256.safetensors', 'control-lora-depth-rank256.safetensors']\n",
        "    Recolor = ['https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-recolor-rank256.safetensors', 'control-lora-recolor-rank256.safetensors']\n",
        "    Sketch = ['https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-sketch-rank256.safetensors', 'control-lora-sketch-rank256.safetensors']\n",
        "    OpenPoseXL2 = ['https://huggingface.co/thibaud/controlnet-openpose-sdxl-1.0/resolve/main/control-lora-openposeXL2-rank256.safetensors', 'control-lora-openposeXL2-rank256.safetensors']\n",
        "\n",
        "class ControlnetModel_SD15(Enum):\n",
        "    Canny = ['https://huggingface.co/lllyasviel/control_v11p_sd15_canny/resolve/main/diffusion_pytorch_model.fp16.safetensors', 'control_v11p_sd15_canny.fp16.safetensors']\n",
        "    Depth = ['https://huggingface.co/lllyasviel/control_v11f1p_sd15_depth/resolve/main/diffusion_pytorch_model.fp16.safetensors', 'control_v11f1p_sd15_depth.fp16.safetensors']\n",
        "    SoftEdge = ['https://huggingface.co/lllyasviel/control_v11p_sd15_softedge/resolve/main/diffusion_pytorch_model.fp16.safetensors', 'control_v11p_sd15_softedge.fp16.safetensors']\n",
        "    Inpaint = ['https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/diffusion_pytorch_model.fp16.safetensors', 'control_v11p_sd15_inpaint.fp16.safetensors']\n",
        "    OpenPose = ['https://huggingface.co/lllyasviel/control_v11p_sd15_openpose/resolve/main/diffusion_pytorch_model.fp16.safetensors', 'control_v11p_sd15_openpose.fp16.safetensors']\n",
        "    Scribble = ['https://huggingface.co/lllyasviel/control_v11p_sd15_scribble/resolve/main/diffusion_pytorch_model.fp16.safetensors', 'control_v11p_sd15_scribble.fp16.safetensors']\n",
        "    LineArt  = ['https://huggingface.co/lllyasviel/control_v11p_sd15_lineart/resolve/main/diffusion_pytorch_model.fp16.safetensors', 'control_v11p_sd15_lineart.fp16.safetensors']\n",
        "\n",
        "class ControlnetModel_XL(Enum):\n",
        "    Canny = ['https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0/resolve/main/diffusion_pytorch_model.fp16.safetensors', 'controlnet-canny-sdxl-1.0.fp16.safetensors']\n",
        "    Depth = ['https://huggingface.co/diffusers/controlnet-depth-sdxl-1.0/resolve/main/diffusion_pytorch_model.fp16.safetensors', 'controlnet-depth-sdxl-1.0.fp16.safetensors']\n",
        "\n",
        "class HyperLoRa(Enum):\n",
        "    HyperSD_15_1_step = ['https://huggingface.co/ByteDance/Hyper-SD/resolve/main/Hyper-SD15-1step-lora.safetensors', 'Hyper-SD15-1step-lora.safetensors']\n",
        "    HyperSD_15_2_steps = ['https://huggingface.co/ByteDance/Hyper-SD/resolve/main/Hyper-SD15-2steps-lora.safetensors', 'Hyper-SD15-2steps-lora.safetensors']\n",
        "    HyperSD_15_4_steps = ['https://huggingface.co/ByteDance/Hyper-SD/resolve/main/Hyper-SD15-4steps-lora.safetensors', 'Hyper-SD15-4steps-lora.safetensors']\n",
        "    HyperSD_15_8_steps = ['https://huggingface.co/ByteDance/Hyper-SD/resolve/main/Hyper-SD15-8steps-lora.safetensors', 'Hyper-SD15-8steps-lora.safetensors']\n",
        "    HyperSD_XL_1_step = ['https://huggingface.co/ByteDance/Hyper-SD/resolve/main/Hyper-SDXL-1step-lora.safetensors', 'Hyper-SDXL-1step-lora.safetensors']\n",
        "    HyperSD_XL_2_steps = ['https://huggingface.co/ByteDance/Hyper-SD/resolve/main/Hyper-SDXL-2steps-lora.safetensors', 'Hyper-SDXL-2steps-lora.safetensors']\n",
        "    HyperSD_XL_4_steps = ['https://huggingface.co/ByteDance/Hyper-SD/resolve/main/Hyper-SDXL-4steps-lora.safetensors', 'Hyper-SDXL-4steps-lora.safetensors']\n",
        "    HyperSD_XL_8_steps = ['https://huggingface.co/ByteDance/Hyper-SD/resolve/main/Hyper-SDXL-8steps-lora.safetensors', 'Hyper-SDXL-8steps-lora.safetensors']\n",
        "    Hyper_Flux_DEV_8_steps = ['https://huggingface.co/ByteDance/Hyper-SD/resolve/main/Hyper-FLUX.1-dev-8steps-lora.safetensors', 'Hyper-FLUX.1-dev-8steps-lora.safetensors']\n",
        "    Hyper_Flux_DEV_16_steps = ['https://huggingface.co/ByteDance/Hyper-SD/resolve/main/Hyper-FLUX.1-dev-16steps-lora.safetensors', 'Hyper-FLUX.1-dev-16steps-lora.safetensors']\n",
        "\n",
        "class UpscalerModel(Enum):\n",
        "    RealESRGAN_x2 = ['https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x2.pth', 'RealESRGAN_x2.pth']\n",
        "    UltraSharp_4x = ['https://huggingface.co/uwg/upscaler/resolve/main/ESRGAN/4x-UltraSharp.pth', '4x-UltraSharp.pth']\n"
      ],
      "metadata": {
        "id": "lh8oeyhWnruO"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nodes to method"
      ],
      "metadata": {
        "id": "u6xXtx6glPfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/{gnn}\n",
        "\n",
        "from enum import Enum\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import nodes\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "if Flux_mode:\n",
        "    from totoro_extras import nodes_custom_sampler, nodes_upscale_model\n",
        "    from totoro import model_management\n",
        "else:\n",
        "    from kmui_extras import nodes_custom_sampler, nodes_upscale_model\n",
        "    from kmui import model_management\n",
        "\n",
        "def closestNumber(n, m):\n",
        "    q = int(n / m)\n",
        "    n1 = m * q\n",
        "    if (n * m) > 0:\n",
        "        n2 = m * (q + 1)\n",
        "    else:\n",
        "        n2 = m * (q - 1)\n",
        "    if abs(n - n1) < abs(n - n2):\n",
        "        return n1\n",
        "    return n2\n",
        "\n",
        "\n",
        "class Scheduler(Enum):\n",
        "    SIMPLE = 'simple'\n",
        "    NORMAL = 'normal'\n",
        "    KARRAS = 'karras'\n",
        "    EXPONENTIAL = 'exponential'\n",
        "    SGM_UNIFORM = 'sgm_uniform'\n",
        "\n",
        "\n",
        "class Sampler(Enum):\n",
        "    DDIM = 'ddim'\n",
        "    Euler = 'euler'\n",
        "    Euler_a = 'euler_ancestral'\n",
        "    DDPM = 'ddpm'\n",
        "    DPM_PP_2M = 'dpmpp_2m'\n",
        "    DPM_PP_2M_SDE = 'dpmpp_2m_sde'\n",
        "    DPM_PP_SDE = 'dpmpp_sde'\n",
        "    DPM2 = 'dpm_2'\n",
        "    DPM2_a = 'dpm_2_ancestral'\n",
        "    Heun = 'heun'\n",
        "    LMS = 'lms'\n",
        "    DEIS = 'deis'\n",
        "    UniPC = 'uni_pc'\n",
        "    LCM = 'lcm'\n",
        "\n",
        "def scale_by_model(pixels, upscale_model:UpscalerModel, scale:float=1, upscale_method=\"nearest-exact\"): # return upscaled pixels\n",
        "    # upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"lanczos\"]\n",
        "    if not os.path.exists(modelpaths.upscale + '/' + upscale_model.value[1]):\n",
        "        download(upscale_model.value[0], upscale_model.value[1], modelpaths.upscale)\n",
        "        if not os.path.exists(modelpaths.upscale + '/' + upscale_model.value[1]):\n",
        "            raise Exception(f'download {upscale_model.value[1]} failed!')\n",
        "\n",
        "    UpscaleModelLoader = nodes_upscale_model.NODE_CLASS_MAPPINGS[\"UpscaleModelLoader\"]()\n",
        "    ImageUpscaleWithModel = nodes_upscale_model.NODE_CLASS_MAPPINGS[\"ImageUpscaleWithModel\"]()\n",
        "    ImageScaleBy = NODE_CLASS_MAPPINGS[\"ImageScaleBy\"]()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        model = UpscaleModelLoader.load_model(model_name=upscale_model.value[1])[0]\n",
        "        image = pixels\n",
        "        if model:\n",
        "            image = ImageUpscaleWithModel.upscale(model, pixels)[0]\n",
        "\n",
        "        if scale != 1:\n",
        "            image = ImageScaleBy.upscale(image, upscale_method, scale)[0]\n",
        "\n",
        "        return image\n",
        "\n",
        "def apply_hyper_lora(unet, lora:HyperLoRa):\n",
        "    if not os.path.exists(modelpaths.lora + '/' + lora.value[1]):\n",
        "        download(lora.value[0], lora.value[1], modelpaths.lora)\n",
        "        if not os.path.exists(modelpaths.lora + '/' + lora.value[1]):\n",
        "            raise Exception(f'download {lora.value[1]} failed!')\n",
        "\n",
        "    LoraLoaderModelOnly = NODE_CLASS_MAPPINGS[\"LoraLoaderModelOnly\"]()\n",
        "    final_model = unet\n",
        "    with torch.inference_mode():\n",
        "        return LoraLoaderModelOnly.load_lora_model_only(final_model, lora.value[1], 1.0)[0]\n",
        "\n",
        "\n",
        "def apply_lora(unet, lora=[]): # lora = [(lora name, strength), ...]\n",
        "    LoraLoaderModelOnly = NODE_CLASS_MAPPINGS[\"LoraLoaderModelOnly\"]()\n",
        "    final_model = unet\n",
        "    with torch.inference_mode():\n",
        "        for it in lora:\n",
        "            final_model = LoraLoaderModelOnly.load_lora_model_only(final_model, it[0], it[1])[0]\n",
        "        return final_model\n",
        "\n",
        "def load_checkpoint(ckpt_name: str):\n",
        "    CheckpointLoaderSimple = NODE_CLASS_MAPPINGS[\"CheckpointLoaderSimple\"]()\n",
        "    with torch.inference_mode():\n",
        "        checkpoint_loader_simple = CheckpointLoaderSimple.load_checkpoint(ckpt_name) # it return (model_patcher, clip, vae, clipvision)\n",
        "        clip = checkpoint_loader_simple[1]\n",
        "        unet = checkpoint_loader_simple[0]\n",
        "        vae = checkpoint_loader_simple[2]\n",
        "        return unet, clip, vae\n",
        "\n",
        "def load_flux():\n",
        "    if not os.path.exists(modelpaths.unet + '/' + 'flux1-dev-fp8.safetensors'):\n",
        "        download('https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/flux1-dev-fp8.safetensors', 'flux1-dev-fp8.safetensors', modelpaths.unet)\n",
        "        download('https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/ae.sft', 'ae.sft', modelpaths.vae)\n",
        "        download('https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/clip_l.safetensors', 'clip_l.safetensors', modelpaths.clip)\n",
        "        download('https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/t5xxl_fp8_e4m3fn.safetensors', 't5xxl_fp8_e4m3fn.safetensors', modelpaths.clip)\n",
        "\n",
        "\n",
        "    DualCLIPLoader = NODE_CLASS_MAPPINGS[\"DualCLIPLoader\"]()\n",
        "    UNETLoader = NODE_CLASS_MAPPINGS[\"UNETLoader\"]()\n",
        "    VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        clip = DualCLIPLoader.load_clip(\"t5xxl_fp8_e4m3fn.safetensors\", \"clip_l.safetensors\", \"flux\")[0]\n",
        "        unet = UNETLoader.load_unet(\"flux1-dev-fp8.safetensors\", \"fp8_e4m3fn\")[0]\n",
        "        vae = VAELoader.load_vae(\"ae.sft\")[0]\n",
        "        return unet, clip, vae\n",
        "\n",
        "def encode_prompt(clip, prompt):\n",
        "    with torch.inference_mode():\n",
        "        cond, pooled = clip.encode_from_tokens(clip.tokenize(prompt), return_pooled=True)\n",
        "        return [[cond, {\"pooled_output\": pooled}]]\n",
        "\n",
        "\n",
        "def create_empty_latent(width, height, batch_size=1):\n",
        "    EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        return EmptyLatentImage.generate(closestNumber(width, 16), closestNumber(height, 16), batch_size=batch_size)[0]\n",
        "\n",
        "def ksampler(model, seed, steps, cfg, sampler: Sampler, scheduler: Scheduler, positive, negative, latent, denoise=1.0):\n",
        "    with torch.inference_mode():\n",
        "        return nodes.common_ksampler(model=model, seed=seed, steps=steps, cfg=cfg, sampler_name=sampler.value,\n",
        "                                     scheduler=scheduler.value, positive=positive, negative=negative,\n",
        "                                     latent=latent, denoise=denoise)[0]\n",
        "\n",
        "def ksampler_flux(unet, seed, steps, cfg, sampler: Sampler, scheduler: Scheduler, positive, latent, denoise=1.0):\n",
        "    RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
        "    BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
        "    KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
        "    BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
        "    SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        noise = RandomNoise.get_noise(seed)[0]\n",
        "        guider = BasicGuider.get_guider(unet, positive)[0]\n",
        "        sampler = KSamplerSelect.get_sampler(sampler.value)[0]\n",
        "        sigmas = BasicScheduler.get_sigmas(unet, scheduler.value, steps, 1.0)[0]\n",
        "        sample, sample_denoised = SamplerCustomAdvanced.sample(noise, guider, sampler, sigmas, latent)\n",
        "        model_management.soft_empty_cache()\n",
        "        return sample\n",
        "\n",
        "\n",
        "def vae_decode(vae, latent): # return image (float[0-1])\n",
        "    VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        return VAEDecode.decode(vae, latent)[0].detach()\n",
        "\n",
        "def vae_encode(vae, pixels): # pixels (float[0-1])\n",
        "    VAEEncode = NODE_CLASS_MAPPINGS[\"VAEEncode\"]()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        return VAEEncode.encode(vae, pixels)[0]\n",
        "\n",
        "def load_image(image_path): # RETURN_TYPES = (\"IMAGE\", \"MASK\")\n",
        "    LoadImage = NODE_CLASS_MAPPINGS[\"LoadImage\"]()\n",
        "    return LoadImage.load_image(image_path)[0]\n",
        "\n",
        "def get_printable_image(image, index=0): # image in float format\n",
        "    return Image.fromarray(np.array(image*255, dtype=np.uint8)[index])\n",
        "\n",
        "def saveJPEG(image, index=0, path='/content/KMUI/output', name='image', quality=90): # image in float format\n",
        "    img = Image.fromarray(np.array(decoded*255, dtype=np.uint8)[index])\n",
        "    img.convert('RGB').save(f'{path}/{name}.jpg', optimize=True, quality=quality)\n",
        "    return img\n"
      ],
      "metadata": {
        "id": "PrC4_OZuEHNs",
        "outputId": "c89efcb5-7e71-448e-a085-b25b97b94d54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "8a2bb5a7e542422798c16ca1027e8327",
            "950439fb933b4f83847fb2f8e36bcc81",
            "5635cc98c9694c0b83e22159739bc3c5",
            "4d516719aa204852abecf5f36bc35f21",
            "77d5e60dcf1b415c84bb79a110fa2dcc",
            "1a2e976f1cc74ed19bbe680d15aa6636",
            "9c6bb1ef4f3b4cabb64bc8f9418c5c00",
            "f1a8b24e21f14547b4d1393421dbfb3c",
            "6ad3b407cb52463ca6ec926fdd7d68ad",
            "30b0cd061d29465d980fcf2f484dddfa",
            "673e0b93210c41c3a12e9001d897ea32"
          ]
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KMUI\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:ERROR: \n",
            "Need to compile C++ extensions to get sparse attention suport.Please run python setup.py build develop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a2bb5a7e542422798c16ca1027e8327"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## custom nodes"
      ],
      "metadata": {
        "id": "9DuIoxttIxBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReActorFaceSwap:\n",
        "    reactor = None\n",
        "\n",
        "    def __init__(self):\n",
        "        from custom_nodes.comfyui_reactor_node.nodes import NODE_CLASS_MAPPINGS\n",
        "\n",
        "        self.reactor = NODE_CLASS_MAPPINGS[\"ReActorFaceSwap\"]()\n",
        "\n",
        "\n",
        "\n",
        "reActorFaceSwap = None\n",
        "if ReactorNode:\n",
        "    reActorFaceSwap = ReActorFaceSwap()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lhjEW5-RIwHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference data"
      ],
      "metadata": {
        "id": "F3pLYafjlXZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ai_prompt, out = promptGenerator.generate_prompt(prompt = 'create image prompt about realistic photo of blonde girl that open her vagina, hot and sexy, small breast size and skinny, open her leg', seed=4)\n"
      ],
      "metadata": {
        "id": "B3_2y8phwXYR"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "promptGenerator.clear_history()\n",
        "print(out['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "id": "q5j07DfDTVoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_prompt = ai_prompt['positive']\n",
        "negative_prompt = ai_prompt['negative']\n",
        "width = 512\n",
        "height = 768\n",
        "seed = 0\n",
        "steps = 10\n",
        "cfg = 0.85\n",
        "sampler = Sampler.DDIM\n",
        "scheduler = Scheduler.SGM_UNIFORM\n",
        "print(positive_prompt)\n",
        "print(negative_prompt)\n",
        "print(f'seed={seed}')"
      ],
      "metadata": {
        "id": "LopCusy3SNzU",
        "outputId": "2d552f01-1f8b-4042-dc0b-29362c51a276",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A blonde girl with small breast size and skinny, sitting on a bed with her legs open, showing her genital area, with a hot and sexy expression, in a high-quality, photorealistic image with soft tones and sharp focus.\n",
            "extra body parts, distorted face, bad anatomy, extra limbs, fused fingers, extra heads, ugly features, watermark, signature, artist name, twitter username, low quality, poor anatomy, malformed limbs.\n",
            "seed=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "GbPYUikl_Udj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unet, clip, vae = load_checkpoint('RealCartoon-Realistic_v17.safetensors')\n",
        "# unet, clip, vae = load_flux()"
      ],
      "metadata": {
        "id": "sCYSnmdx_ZJ1",
        "outputId": "de39017a-3bb3-4f98-fa34-7e27f8724176",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple text to image"
      ],
      "metadata": {
        "id": "NS8f_7E5lseA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "seed = random.randint(0, 18446744073709551615)\n",
        "empty_latent = create_empty_latent(width,height)\n",
        "unet2 = apply_hyper_lora(unet, HyperLoRa.HyperSD_15_8_steps)\n",
        "# unet2 = apply_lora(unet2,\n",
        "#                    lora=[\n",
        "#                        ['WowifierXL.safetensors', 0.7],\n",
        "#                    ])\n",
        "latent = ksampler(unet2, seed, steps, cfg, sampler, scheduler,\n",
        "                  encode_prompt(clip, positive_prompt), encode_prompt(clip, negative_prompt),\n",
        "                  empty_latent)\n",
        "image = vae_decode(vae, latent)\n",
        "img = get_printable_image(image)\n",
        "clear_output()\n",
        "# saveJPEG(image)\n",
        "print(f'seed={seed}')\n",
        "img"
      ],
      "metadata": {
        "id": "TgGY5TgwQYqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple text to image Flux.1 DEX"
      ],
      "metadata": {
        "id": "VbVrHjyy1QOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "empty_latent = create_empty_latent(width,height)\n",
        "unet2 = apply_hyper_lora(unet, HyperLoRa.Hyper_Flux_DEV_8_steps)\n",
        "latent = ksampler_flux(unet2, seed, steps, cfg, Sampler.Euler, Scheduler.SIMPLE,\n",
        "                  encode_prompt(clip, positive_prompt), empty_latent)\n",
        "image = vae_decode(vae, latent)\n",
        "img = get_printable_image(image)\n",
        "# saveJPEG(image)\n",
        "print(f'seed={seed}')\n",
        "img"
      ],
      "metadata": {
        "id": "xcFxnUUm1Q9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple image to image"
      ],
      "metadata": {
        "id": "qYRTxq85lwyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if seed == 0:\n",
        "    seed = random.randint(0, 18446744073709551615)\n",
        "print(f'seed={seed}')\n",
        "\n",
        "pixels = load_image('example.png')\n",
        "latent = vae_encode(vae, pixels)\n",
        "latent = ksampler(unet, seed, steps, cfg, sampler, scheduler,\n",
        "                  encode_prompt(clip, positive_prompt), encode_prompt(clip, negative_prompt),\n",
        "                  latent, denoise=0.8)\n",
        "image = vae_decode(vae, latent)\n",
        "img = get_printable_image(image)\n",
        "img"
      ],
      "metadata": {
        "id": "eHbGqwBVcrkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UpScale Image"
      ],
      "metadata": {
        "id": "OaYV1yCpKOlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_nX = scale_by_model(image, UpscalerModel.RealESRGAN_x2, scale=1)\n",
        "latent = vae_encode(vae, image_nX)\n",
        "latent = ksampler(unet2, seed, steps, cfg, sampler, scheduler,\n",
        "                  encode_prompt(clip, positive_prompt), encode_prompt(clip, negative_prompt),\n",
        "                  latent, denoise=0.4)\n",
        "image_nX = vae_decode(vae, latent)\n",
        "img = get_printable_image(image_nX)\n",
        "img"
      ],
      "metadata": {
        "id": "o7GfOEAUGeyV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "AyH4Jb5mNofH",
        "USPvp_jieoRL",
        "vUabsHbSTKF6",
        "Nb8G2WauywJu"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8a2bb5a7e542422798c16ca1027e8327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_950439fb933b4f83847fb2f8e36bcc81",
              "IPY_MODEL_5635cc98c9694c0b83e22159739bc3c5",
              "IPY_MODEL_4d516719aa204852abecf5f36bc35f21"
            ],
            "layout": "IPY_MODEL_77d5e60dcf1b415c84bb79a110fa2dcc"
          }
        },
        "950439fb933b4f83847fb2f8e36bcc81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a2e976f1cc74ed19bbe680d15aa6636",
            "placeholder": "​",
            "style": "IPY_MODEL_9c6bb1ef4f3b4cabb64bc8f9418c5c00",
            "value": ""
          }
        },
        "5635cc98c9694c0b83e22159739bc3c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1a8b24e21f14547b4d1393421dbfb3c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ad3b407cb52463ca6ec926fdd7d68ad",
            "value": 0
          }
        },
        "4d516719aa204852abecf5f36bc35f21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30b0cd061d29465d980fcf2f484dddfa",
            "placeholder": "​",
            "style": "IPY_MODEL_673e0b93210c41c3a12e9001d897ea32",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "77d5e60dcf1b415c84bb79a110fa2dcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a2e976f1cc74ed19bbe680d15aa6636": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c6bb1ef4f3b4cabb64bc8f9418c5c00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1a8b24e21f14547b4d1393421dbfb3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "6ad3b407cb52463ca6ec926fdd7d68ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "30b0cd061d29465d980fcf2f484dddfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "673e0b93210c41c3a12e9001d897ea32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}