{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "80027838751f4cbe88ed0492c86f07ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SelectModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SelectModel",
            "_options_labels": [
              "Stable Diffusion v1.5",
              "Dark Sushi Mix",
              "ExpMix Line",
              "ChameleonAiMix",
              "majicMIX realistic",
              "OrangeMixs",
              "HenMixReal",
              "DreamlikePhotoreal",
              "NeverEndingDream2",
              "F2",
              "PerfectWorldV4B",
              "DreamShaper",
              "AnyLoRA",
              "AbsoluteReality",
              "MeinaHentai",
              "KenCanMix",
              "HenmixArt",
              "Reliberate",
              "Deliberate",
              "HenmixReal",
              "RunDiffusionFXPhotorealistic",
              "Paragon-V1.0",
              "Photon",
              "RealisticVision",
              "SDXL-V1.0 Base",
              "SDXL-V1.0 Refiner",
              "SDVN6-RealXL",
              "DynaVisionXL",
              "RealCartoon-Realistic",
              "CounterfeitXL-anime",
              "ZavyChromaXL",
              "DreamShaperXL1.0",
              "3DAnimationDiffusion",
              "FaeTastic",
              "naturalsin",
              "JuggernautXL",
              "StarlightXL"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "SelectView",
            "description": "Choose a model:",
            "description_tooltip": null,
            "disabled": false,
            "index": 23,
            "layout": "IPY_MODEL_2f089a3baafa4f76a4a0e956acf64a9e",
            "rows": 5,
            "style": "IPY_MODEL_eee7deb6025a4ef08bc5defd6d5e093c"
          }
        },
        "2f089a3baafa4f76a4a0e956acf64a9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eee7deb6025a4ef08bc5defd6d5e093c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        },
        "e6806f6ec12d4b879e06de8b0e3d271e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SelectModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SelectModel",
            "_options_labels": [
              "Realistic_Vision_V5.0.safetensors",
              "Realistic_Vision_V5.0-inpainting.safetensors"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "SelectView",
            "description": "Choose a model version:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_1928c576f68a4cb0825c6916c0a47a5e",
            "rows": 5,
            "style": "IPY_MODEL_943343e861724125b4649d33a138cc47"
          }
        },
        "1928c576f68a4cb0825c6916c0a47a5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "943343e861724125b4649d33a138cc47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "initial"
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamranr123/kamranr123.github.io/blob/master/SD_diffusers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> KM Colab</h1>"
      ],
      "metadata": {
        "id": "Ci1lmgVY-TaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## Choose Models\n",
        "import markdown\n",
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "from importlib import reload\n",
        "\n",
        "# ******************************************************************************\n",
        "class Modelpaths:\n",
        "    base_path = '/content/models'\n",
        "    model = f'{base_path}/checkpoints'\n",
        "    lora = f'{base_path}/loras'\n",
        "    vae = f'{base_path}/vae'\n",
        "    upscale = f'{base_path}/upscale_models'\n",
        "    controlnet = f'{base_path}/controlnet'\n",
        "    embeddings = f'{base_path}/embeddings'\n",
        "    diffusers = f'{base_path}/diffusers'\n",
        "\n",
        "    def __init__(self):\n",
        "        if not os.path.exists(self.base_path):\n",
        "            os.makedirs(self.model)\n",
        "            os.makedirs(self.lora)\n",
        "            os.makedirs(self.vae)\n",
        "            os.makedirs(self.upscale)\n",
        "            os.makedirs(self.embeddings)\n",
        "            os.makedirs(self.diffusers)\n",
        "\n",
        "modelpaths = Modelpaths()\n",
        "\n",
        "%cd /content\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "def download(model_link, model_name, path = ''):\n",
        "    # !cd /content/gdrive/MyDrive/yourfoldername; wget civitai-link --content-disposition\n",
        "    if path == '':\n",
        "        path = modelpaths.model\n",
        "\n",
        "    %cd {path}\n",
        "    if path == modelpaths.model:\n",
        "        !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {model_link} --dir={path} --out={model_name}\n",
        "    else:\n",
        "        !aria2c --console-log-level=error -c -x 16 -s 8 -k 1M {model_link} --dir={path} --out={model_name}\n",
        "# ******************************************************************************\n",
        "clear_output()\n",
        "\n",
        "!wget -q https://github.com/kamranr123/upscaler/raw/main/upscale.py -P /content/\n",
        "import upscale\n",
        "reload(upscale)\n",
        "\n",
        "# ******************************************************************************\n",
        "!wget -q https://raw.githubusercontent.com/kamranr123/kamranr123.github.io/master/modellist.py -P /content/\n",
        "import modellist\n",
        "reload(modellist)\n",
        "available_model_dict = modellist.available_model_dict\n",
        "# ******************************************************************************\n",
        "# initialize values\n",
        "model = list(available_model_dict.keys())[0]\n",
        "model_ver = available_model_dict[model][0]\n",
        "model_name = available_model_dict[model][4]\n",
        "model_link = available_model_dict[model][3] + model_name\n",
        "if 'civitai.com' in model_link:\n",
        "    model_link = available_model_dict[model][3]\n",
        "\n",
        "# Define the title and link\n",
        "title = \"Model selection\"\n",
        "link = \"https://github.com/NUROISEA/anime-webui-colab/wiki/Selecting-a-model\"\n",
        "\n",
        "# Generate the Markdown-formatted text\n",
        "markdown_text = f\"# {title} [**[?]**]({link})\"\n",
        "\n",
        "# Convert the Markdown text to HTML\n",
        "html = markdown.markdown(markdown_text)\n",
        "display(Markdown(html))\n",
        "\n",
        "# Create a list of models for the first selection listbox\n",
        "model_options = list(available_model_dict.keys())\n",
        "\n",
        "# Create the first selection listbox widget for models\n",
        "model_selection = widgets.Select(\n",
        "    options=model_options,\n",
        "    description='Choose a model:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "# Create the second selection listbox widget for values\n",
        "model_version_selection = widgets.Select(\n",
        "    options = available_model_dict[model_options[0]][4:],\n",
        "    description='Choose a model version:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "# Define a function to handle the model selection change event\n",
        "def on_model_selection_change(change):\n",
        "    global model\n",
        "    model = change['new']\n",
        "    print('change')\n",
        "    selected_values = available_model_dict[model][4:]\n",
        "    model_version_selection.options = selected_values\n",
        "\n",
        "# Attach the model selection change event handler\n",
        "model_selection.observe(on_model_selection_change, 'value')\n",
        "\n",
        "# Define a function to handle the model selection change event\n",
        "def on_model_version_selection_change(change):\n",
        "    global model_name, model_link, model_ver\n",
        "    model_name = change['new']\n",
        "    model_link = available_model_dict[model][3] + model_name\n",
        "    model_ver = available_model_dict[model][0]\n",
        "    if 'civitai.com' in model_link:\n",
        "        model_link = available_model_dict[model][3]\n",
        "    print(\"\\r\", end=\"\")\n",
        "    print(\"Selected Model: {}, Version: {}\".format(model, model_name), end=\"\")\n",
        "    # print(\"Selected Model link: {}\".format(model_link), end=\"\")\n",
        "\n",
        "# Attach the model version selection change event handler\n",
        "model_version_selection.observe(on_model_version_selection_change, 'value')\n",
        "\n",
        "# Display the selection listboxes\n",
        "display(model_selection)\n",
        "display(model_version_selection)\n",
        "print(\"Selected Model: {}, Version: {}\".format(model, model_name), end=\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293,
          "referenced_widgets": [
            "80027838751f4cbe88ed0492c86f07ed",
            "2f089a3baafa4f76a4a0e956acf64a9e",
            "eee7deb6025a4ef08bc5defd6d5e093c",
            "e6806f6ec12d4b879e06de8b0e3d271e",
            "1928c576f68a4cb0825c6916c0a47a5e",
            "943343e861724125b4649d33a138cc47"
          ]
        },
        "cellView": "form",
        "id": "LHQQHnqq-gvO",
        "outputId": "280f3d1a-48bf-4723-82b2-ddd7c222b833"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<h1>Model selection <a href=\"https://github.com/NUROISEA/anime-webui-colab/wiki/Selecting-a-model\"><strong>[?]</strong></a></h1>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Select(description='Choose a model:', options=('Stable Diffusion v1.5', 'Dark Sushi Mix', 'ExpMix Line', 'Cham…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80027838751f4cbe88ed0492c86f07ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Select(description='Choose a model version:', options=('v1-5-pruned.safetensors', 'v1-5-pruned-emaonly.safeten…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6806f6ec12d4b879e06de8b0e3d271e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Model: Stable Diffusion v1.5, Version: v1-5-pruned.safetensorschange\n",
            "Selected Model: RealisticVision, Version: Realistic_Vision_V5.0.safetensors"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download(model_link, model_name, modelpaths.model)\n",
        "\n",
        "!wget -q 'https://huggingface.co/nolanaatama/colab/resolve/main/embeddings.zip' -P {modelpaths.embeddings}\n",
        "with zipfile.ZipFile(f\"{modelpaths.embeddings}/embeddings.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(f\"{modelpaths.base_path}\")\n",
        "os.remove(f\"{modelpaths.embeddings}/embeddings.zip\")\n",
        "\n",
        "download('https://civitai.com/api/download/models/218471', 'blackgold-000008.safetensors', modelpaths.lora)\n",
        "download('https://civitai.com/api/download/models/87153?type=Model&format=SafeTensor', 'more_details.safetensors', modelpaths.lora)\n"
      ],
      "metadata": {
        "id": "rbirrgcUHFDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial"
      ],
      "metadata": {
        "id": "b3anNC1QP77l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers transformers accelerate omegaconf peft\n",
        "# !pip install xformers==0.0.2\n",
        "import torch\n",
        "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
        "from diffusers import StableDiffusionImg2ImgPipeline, StableDiffusionPipeline, AutoencoderKL\n",
        "from diffusers import EulerDiscreteScheduler, EulerAncestralDiscreteScheduler, DPMSolverMultistepScheduler\n",
        "from diffusers import DPMSolverSinglestepScheduler, KDPM2DiscreteScheduler, KDPM2AncestralDiscreteScheduler\n",
        "from diffusers import HeunDiscreteScheduler, LMSDiscreteScheduler, DEISMultistepScheduler, UniPCMultistepScheduler\n",
        "\n",
        "from diffusers.utils import load_image\n",
        "# example image = load_image(\"https://user-images.githubusercontent.com/24734142/266492875-2d50d223-8475-44f0-a7c6-08b51cb53572.png\")\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "BVzUL0t1Sak5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install ip-adapter\n",
        "!pip install git+https://github.com/tencent-ailab/IP-Adapter.git\n",
        "# download the models\n",
        "!cd IP-Adapter\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/h94/IP-Adapter\n",
        "!mv IP-Adapter/models models\n",
        "!mv IP-Adapter/sdxl_models sdxl_models"
      ],
      "metadata": {
        "id": "YXWUfk80miD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "import math\n",
        "import safetensors\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# clip_text_custom_embedder.py\n",
        "# https://gist.github.com/takuma104/43552b8ec70b63323c57dc9c6fcb9b90\n",
        "\n",
        "re_attention = re.compile(r\"\"\"\n",
        "\\\\\\(|\n",
        "\\\\\\{|\n",
        "\\\\\\)|\n",
        "\\\\\\}|\n",
        "\\\\\\[|\n",
        "\\\\]|\n",
        "\\\\\\\\|\n",
        "\\\\|\n",
        "\\(|\n",
        "\\{|\n",
        "\\[|\n",
        ":([+-]?[.\\d]+)\\)|\n",
        "\\)|\n",
        "\\}|\n",
        "]|\n",
        "[^\\\\()\\\\{}\\[\\]:]+|\n",
        ":\n",
        "\"\"\", re.X)\n",
        "\n",
        "\n",
        "def parse_prompt_attention(text):\n",
        "    \"\"\"\n",
        "    Parses a string with attention tokens and returns a list of pairs: text and its assoicated weight.\n",
        "    Accepted tokens are:\n",
        "      (abc) - increases attention to abc by a multiplier of 1.1\n",
        "      (abc:3.12) - increases attention to abc by a multiplier of 3.12\n",
        "      [abc] - decreases attention to abc by a multiplier of 1.1\n",
        "      \\( - literal character '('\n",
        "      \\[ - literal character '['\n",
        "      \\) - literal character ')'\n",
        "      \\] - literal character ']'\n",
        "      \\\\ - literal character '\\'\n",
        "      anything else - just text\n",
        "    >>> parse_prompt_attention('normal text')\n",
        "    [['normal text', 1.0]]\n",
        "    >>> parse_prompt_attention('an (important) word')\n",
        "    [['an ', 1.0], ['important', 1.1], [' word', 1.0]]\n",
        "    >>> parse_prompt_attention('(unbalanced')\n",
        "    [['unbalanced', 1.1]]\n",
        "    >>> parse_prompt_attention('\\(literal\\]')\n",
        "    [['(literal]', 1.0]]\n",
        "    >>> parse_prompt_attention('(unnecessary)(parens)')\n",
        "    [['unnecessaryparens', 1.1]]\n",
        "    >>> parse_prompt_attention('a (((house:1.3)) [on] a (hill:0.5), sun, (((sky))).')\n",
        "    [['a ', 1.0],\n",
        "     ['house', 1.5730000000000004],\n",
        "     [' ', 1.1],\n",
        "     ['on', 1.0],\n",
        "     [' a ', 1.1],\n",
        "     ['hill', 0.55],\n",
        "     [', sun, ', 1.1],\n",
        "     ['sky', 1.4641000000000006],\n",
        "     ['.', 1.1]]\n",
        "    \"\"\"\n",
        "\n",
        "    res = []\n",
        "    round_brackets = []\n",
        "    square_brackets = []\n",
        "\n",
        "    round_bracket_multiplier = 1.1\n",
        "    square_bracket_multiplier = 1 / 1.1\n",
        "\n",
        "    def multiply_range(start_position, multiplier):\n",
        "        for p in range(start_position, len(res)):\n",
        "            res[p][1] *= multiplier\n",
        "\n",
        "    for m in re_attention.finditer(text):\n",
        "        text = m.group(0)\n",
        "        weight = m.group(1)\n",
        "\n",
        "        if text.startswith('\\\\'):\n",
        "            res.append([text[1:], 1.0])\n",
        "        elif text == '(' or text == '{':\n",
        "            round_brackets.append(len(res))\n",
        "        elif text == '[':\n",
        "            square_brackets.append(len(res))\n",
        "        elif weight is not None and len(round_brackets) > 0:\n",
        "            multiply_range(round_brackets.pop(), float(weight))\n",
        "        elif (text == ')' or text == '}') and len(round_brackets) > 0:\n",
        "            multiply_range(round_brackets.pop(), round_bracket_multiplier)\n",
        "        elif text == ']' and len(square_brackets) > 0:\n",
        "            multiply_range(square_brackets.pop(), square_bracket_multiplier)\n",
        "        else:\n",
        "            res.append([text, 1.0])\n",
        "\n",
        "    for pos in round_brackets:\n",
        "        multiply_range(pos, round_bracket_multiplier)\n",
        "\n",
        "    for pos in square_brackets:\n",
        "        multiply_range(pos, square_bracket_multiplier)\n",
        "\n",
        "    if len(res) == 0:\n",
        "        res = [[\"\", 1.0]]\n",
        "\n",
        "    # merge runs of identical weights\n",
        "    i = 0\n",
        "    while i + 1 < len(res):\n",
        "        if res[i][1] == res[i + 1][1]:\n",
        "            res[i][0] += res[i + 1][0]\n",
        "            res.pop(i + 1)\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "class CLIPTextCustomEmbedder(object):\n",
        "    def __init__(self, tokenizer, text_encoder, device,\n",
        "                 clip_stop_at_last_layers=1):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text_encoder = text_encoder\n",
        "        self.token_mults = {}\n",
        "        self.device = device\n",
        "        self.clip_stop_at_last_layers = clip_stop_at_last_layers\n",
        "\n",
        "    def tokenize_line(self, line):\n",
        "        def get_target_prompt_token_count(token_count):\n",
        "            return math.ceil(max(token_count, 1) / 75) * 75\n",
        "\n",
        "        id_end = self.tokenizer.eos_token_id\n",
        "        parsed = parse_prompt_attention(line)\n",
        "        tokenized = self.tokenizer(\n",
        "            [text for text, _ in parsed], truncation=False,\n",
        "            add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "        fixes = []\n",
        "        remade_tokens = []\n",
        "        multipliers = []\n",
        "\n",
        "        for tokens, (text, weight) in zip(tokenized, parsed):\n",
        "            i = 0\n",
        "            while i < len(tokens):\n",
        "                token = tokens[i]\n",
        "                remade_tokens.append(token)\n",
        "                multipliers.append(weight)\n",
        "                i += 1\n",
        "\n",
        "        token_count = len(remade_tokens)\n",
        "        prompt_target_length = get_target_prompt_token_count(token_count)\n",
        "        tokens_to_add = prompt_target_length - len(remade_tokens)\n",
        "        remade_tokens = remade_tokens + [id_end] * tokens_to_add\n",
        "        multipliers = multipliers + [1.0] * tokens_to_add\n",
        "        return remade_tokens, fixes, multipliers, token_count\n",
        "\n",
        "    def process_text(self, texts):\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "\n",
        "        remade_batch_tokens = []\n",
        "        cache = {}\n",
        "        batch_multipliers = []\n",
        "        for line in texts:\n",
        "            if line in cache:\n",
        "                remade_tokens, fixes, multipliers = cache[line]\n",
        "            else:\n",
        "                remade_tokens, fixes, multipliers, _ = self.tokenize_line(line)\n",
        "                cache[line] = (remade_tokens, fixes, multipliers)\n",
        "\n",
        "            remade_batch_tokens.append(remade_tokens)\n",
        "            batch_multipliers.append(multipliers)\n",
        "\n",
        "        return batch_multipliers, remade_batch_tokens\n",
        "\n",
        "    def __call__(self, text):\n",
        "        batch_multipliers, remade_batch_tokens = self.process_text(text)\n",
        "\n",
        "        z = None\n",
        "        i = 0\n",
        "        while max(map(len, remade_batch_tokens)) != 0:\n",
        "            rem_tokens = [x[75:] for x in remade_batch_tokens]\n",
        "            rem_multipliers = [x[75:] for x in batch_multipliers]\n",
        "\n",
        "            tokens = []\n",
        "            multipliers = []\n",
        "            for j in range(len(remade_batch_tokens)):\n",
        "                if len(remade_batch_tokens[j]) > 0:\n",
        "                    tokens.append(remade_batch_tokens[j][:75])\n",
        "                    multipliers.append(batch_multipliers[j][:75])\n",
        "                else:\n",
        "                    tokens.append([self.tokenizer.eos_token_id] * 75)\n",
        "                    multipliers.append([1.0] * 75)\n",
        "\n",
        "            z1 = self.process_tokens(tokens, multipliers)\n",
        "            z = z1 if z is None else torch.cat((z, z1), axis=-2)\n",
        "\n",
        "            remade_batch_tokens = rem_tokens\n",
        "            batch_multipliers = rem_multipliers\n",
        "            i += 1\n",
        "\n",
        "        return z\n",
        "\n",
        "    def process_tokens(self, remade_batch_tokens, batch_multipliers):\n",
        "        remade_batch_tokens = [[self.tokenizer.bos_token_id] + x[:75] +\n",
        "                               [self.tokenizer.eos_token_id] for x in remade_batch_tokens]\n",
        "        batch_multipliers = [[1.0] + x[:75] + [1.0] for x in batch_multipliers]\n",
        "\n",
        "        tokens = torch.asarray(remade_batch_tokens).to(self.device)\n",
        "        # print(tokens.shape)\n",
        "        # print(tokens)\n",
        "        outputs = self.text_encoder(\n",
        "            input_ids=tokens, output_hidden_states=True)\n",
        "\n",
        "        if self.clip_stop_at_last_layers > 1:\n",
        "            z = self.text_encoder.text_model.final_layer_norm(\n",
        "                outputs.hidden_states[-self.clip_stop_at_last_layers])\n",
        "        else:\n",
        "            z = outputs.last_hidden_state\n",
        "\n",
        "        # restoring original mean is likely not correct, but it seems to work well\n",
        "        # to prevent artifacts that happen otherwise\n",
        "        batch_multipliers_of_same_length = [\n",
        "            x + [1.0] * (75 - len(x)) for x in batch_multipliers]\n",
        "        batch_multipliers = torch.asarray(\n",
        "            batch_multipliers_of_same_length).to(self.device)\n",
        "        # print(batch_multipliers.shape)\n",
        "        # print(batch_multipliers)\n",
        "\n",
        "        original_mean = z.mean()\n",
        "        z *= batch_multipliers.reshape(batch_multipliers.shape +\n",
        "                                       (1,)).expand(z.shape)\n",
        "        new_mean = z.mean()\n",
        "        z *= original_mean / new_mean\n",
        "\n",
        "        return z\n",
        "\n",
        "    def get_text_tokens(self, text):\n",
        "        batch_multipliers, remade_batch_tokens = self.process_text(text)\n",
        "        return [[self.tokenizer.bos_token_id] + remade_batch_tokens[0]], \\\n",
        "            [[1.0] + batch_multipliers[0]]\n",
        "\n",
        "\n",
        "def text_embeddings_equal_len(text_embedder, prompt, negative_prompt):\n",
        "    cond_embeddings = text_embedder(prompt)\n",
        "    uncond_embeddings = text_embedder(negative_prompt)\n",
        "\n",
        "    cond_len = cond_embeddings.shape[1]\n",
        "    uncond_len = uncond_embeddings.shape[1]\n",
        "    if cond_len == uncond_len:\n",
        "        return cond_embeddings, uncond_embeddings\n",
        "    else:\n",
        "        if cond_len > uncond_len:\n",
        "            n = (cond_len - uncond_len) // 77\n",
        "            return cond_embeddings, torch.cat([uncond_embeddings] + [text_embedder(\"\")]*n, dim=1)\n",
        "        else:\n",
        "            n = (uncond_len - cond_len) // 77\n",
        "            return torch.cat([cond_embeddings] + [text_embedder(\"\")]*n, dim=1), uncond_embeddings\n",
        "\n",
        "\n",
        "def text_embeddings(pipe, prompt, negative_prompt, clip_stop_at_last_layers=1):\n",
        "    text_embedder = CLIPTextCustomEmbedder(tokenizer=pipe.tokenizer,\n",
        "                                           text_encoder=pipe.text_encoder,\n",
        "                                           device=pipe.text_encoder.device,\n",
        "                                           clip_stop_at_last_layers=clip_stop_at_last_layers)\n",
        "    cond_embeddings, uncond_embeddings = text_embeddings_equal_len(text_embedder, prompt, negative_prompt)\n",
        "    return cond_embeddings, uncond_embeddings\n",
        "# *******************************************************\n",
        "\n",
        "def saveJPEG(image, path, name, quality=90):\n",
        "    image.convert('RGB').save(f'{path}/{name}.jpg', optimize=True, quality=quality)\n",
        "\n",
        "class SDLoader:\n",
        "\n",
        "    class UpscalerModel(Enum):\n",
        "        RealESRGAN_x2 = ['https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x2.pth', 'RealESRGAN_x2.pth']\n",
        "        UltraSharp_4x = ['https://huggingface.co/uwg/upscaler/resolve/main/ESRGAN/4x-UltraSharp.pth', '4x-UltraSharp.pth']\n",
        "\n",
        "    class Scheduler(Enum):\n",
        "        Euler = 1\n",
        "        Euler_a = 2\n",
        "        DPM_PP_2M = 3\n",
        "        DPM_PP_2M_SDE = 4\n",
        "        DPM_PP_SDE = 5\n",
        "        DPM2 = 6\n",
        "        DPM2_a = 7\n",
        "        Heun = 8\n",
        "        LMS = 9\n",
        "        DEIS = 10\n",
        "        UniPC = 11\n",
        "\n",
        "    __device_name = \"cuda\"\n",
        "    __torch_dtype = torch.float16\n",
        "    _pipe = None\n",
        "\n",
        "    _vae=None\n",
        "    _original_vae=None\n",
        "    _vae_name=None\n",
        "    _scheduler_type = Scheduler.Euler\n",
        "\n",
        "    __default_clip_skip=1\n",
        "    _clip_skip = __default_clip_skip\n",
        "    _model_diffuser_path=None\n",
        "    __original_clip_layers=None\n",
        "\n",
        "    __loaded_embeddings = []\n",
        "    __loaded_lora = []\n",
        "\n",
        "    _esrgan_model=None\n",
        "    _upscaler_model=None\n",
        "    _upscaler_type=None\n",
        "    seed = 0\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        if not torch.cuda.is_available():\n",
        "            print('cuda is unavailable!')\n",
        "            self.__device_name = torch.device(\"cpu\")\n",
        "            self.__torch_dtype = torch.float32\n",
        "        self._upscaler_model=upscale.UpscaleModel()\n",
        "        pass\n",
        "\n",
        "    def loadModel(self, safetensors_model, clip_skip=2):\n",
        "        self._model_diffuser_path = modelpaths.model + '/' + safetensors_model\n",
        "        if not os.path.exists(self._model_diffuser_path):\n",
        "            print(f\"{safetensors_model} not found !\")\n",
        "            return\n",
        "        if self._pipe is not None:\n",
        "            print(\"unload the last model before continue!\")\n",
        "            return\n",
        "\n",
        "        self._pipe = StableDiffusionPipeline.from_single_file(self._model_diffuser_path,\n",
        "                                    torch_dtype=self.__torch_dtype, load_safety_checker=False)\n",
        "        self._pipe = self._pipe.to(self.__device_name)\n",
        "        # self._pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n",
        "\n",
        "\n",
        "        self._original_vae=self._pipe.vae\n",
        "        self.change_scheduler(self._scheduler_type, True)\n",
        "\n",
        "        self.__original_clip_layers = self._pipe.text_encoder.text_model.encoder.layers\n",
        "        self.setClipSkip(self._clip_skip)\n",
        "        self.reload_embeddings()\n",
        "\n",
        "    def loadVAE(self, vae_name):\n",
        "        path = modelpaths.vae + '/' + vae_name\n",
        "        if not os.path.exists(self.path):\n",
        "            print(f\"{vae_name} not found !\")\n",
        "            return\n",
        "\n",
        "        if vae_name != self._vae_name:\n",
        "            if (vae_name is None) and (self._vae_name is not None):\n",
        "                self._pipe.vae = self._original_vae\n",
        "                self._vae_name = None\n",
        "            elif vae_name is not None:\n",
        "                self._vae = AutoencoderKL.from_single_file(path)\n",
        "                self._vae_name=vae_name\n",
        "                self._pipe.vae = self._vae\n",
        "\n",
        "    def setClipSkip(self, clip_skip):\n",
        "        if self._pipe is None:\n",
        "            print('no model loaded!')\n",
        "            return\n",
        "        self._clip_skip = clip_skip\n",
        "\n",
        "        if clip_skip > 1:\n",
        "            self._pipe.text_encoder.text_model.encoder.layers = self.__original_clip_layers[:-(clip_skip-1)]\n",
        "        else:\n",
        "            self._pipe.text_encoder.text_model.encoder.layers = self.__original_clip_layers\n",
        "\n",
        "    def change_scheduler(self, scheduler: Scheduler, karras=False):\n",
        "        if self._pipe is None:\n",
        "            print('no model loaded!')\n",
        "            return\n",
        "\n",
        "        if scheduler == SDLoader.Scheduler.Euler:\n",
        "            self._pipe.scheduler = EulerDiscreteScheduler.from_config(self._pipe.scheduler.config)\n",
        "        elif scheduler == SDLoader.Scheduler.Euler_a:\n",
        "            self._pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(self._pipe.scheduler.config)\n",
        "        elif scheduler == SDLoader.Scheduler.DPM_PP_2M:\n",
        "            self._pipe.scheduler = DPMSolverMultistepScheduler.from_config(self._pipe.scheduler.config)\n",
        "        elif scheduler == SDLoader.Scheduler.DPM_PP_2M_SDE:\n",
        "            self._pipe.scheduler = DPMSolverMultistepScheduler.from_config(self._pipe.scheduler.config)\n",
        "            self._pipe.scheduler.algorithm_type=\"sde-dpmsolver++\"\n",
        "        elif scheduler == SDLoader.Scheduler.DPM_PP_SDE:\n",
        "            self._pipe.scheduler = DPMSolverSinglestepScheduler.from_config(self._pipe.scheduler.config)\n",
        "            self._pipe.scheduler.algorithm_type=\"sde-dpmsolver++\"\n",
        "        elif scheduler == SDLoader.Scheduler.DPM2:\n",
        "            self._pipe.scheduler = KDPM2DiscreteScheduler.from_config(self._pipe.scheduler.config)\n",
        "        elif scheduler == SDLoader.Scheduler.DPM2_a:\n",
        "            self._pipe.scheduler = KDPM2AncestralDiscreteScheduler.from_config(self._pipe.scheduler.config)\n",
        "        elif scheduler == SDLoader.Scheduler.Heun:\n",
        "            self._pipe.scheduler = HeunDiscreteScheduler.from_config(self._pipe.scheduler.config)\n",
        "        elif scheduler == SDLoader.Scheduler.LMS:\n",
        "            self._pipe.scheduler = LMSDiscreteScheduler.from_config(self._pipe.scheduler.config)\n",
        "        elif scheduler == SDLoader.Scheduler.DEIS:\n",
        "            self._pipe.scheduler = DEISMultistepScheduler.from_config(self._pipe.scheduler.config)\n",
        "        elif scheduler == SDLoader.Scheduler.UniPC:\n",
        "            self._pipe.scheduler = UniPCMultistepScheduler.from_config(self._pipe.scheduler.config)\n",
        "\n",
        "        self._pipe.scheduler.use_karras_sigmas=karras\n",
        "\n",
        "    def unload(self):\n",
        "        if self._pipe is not None:\n",
        "            self._pipe.maybe_free_model_hooks()\n",
        "            self._pipe.unload_lora_weights()\n",
        "            self._pipe = None\n",
        "            self._model_diffuser_path=None\n",
        "            self.__loaded_embeddings=[]\n",
        "            self.__original_clip_layers=None\n",
        "            self.clip_skip=self.__default_clip_skip\n",
        "            self._vae=None\n",
        "            self._original_vae=None\n",
        "            self._vae_name=None\n",
        "\n",
        "    def __get_pipeline_embeds(self, prompt, negative_prompt):\n",
        "        \"\"\" Get pipeline embeds for prompts bigger than the maxlength of the pipe\n",
        "        :param prompt:\n",
        "        :param negative_prompt:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        max_length = self._pipe.tokenizer.model_max_length\n",
        "\n",
        "        # simple way to determine length of tokens\n",
        "        count_prompt = len(prompt.split(\" \"))\n",
        "        count_negative_prompt = len(negative_prompt.split(\" \"))\n",
        "\n",
        "        # create the tensor based on which prompt is longer\n",
        "        if count_prompt >= count_negative_prompt:\n",
        "            input_ids = self._pipe.tokenizer(prompt, return_tensors=\"pt\", truncation=False).input_ids.to(self.__device_name)\n",
        "            shape_max_length = input_ids.shape[-1]\n",
        "            negative_ids = self._pipe.tokenizer(negative_prompt, truncation=False, padding=\"max_length\",\n",
        "                                            max_length=shape_max_length, return_tensors=\"pt\").input_ids.to(self.__device_name)\n",
        "\n",
        "        else:\n",
        "            negative_ids = self._pipe.tokenizer(negative_prompt, return_tensors=\"pt\", truncation=False).input_ids.to(self.__device_name)\n",
        "            shape_max_length = negative_ids.shape[-1]\n",
        "            input_ids = self._pipe.tokenizer(prompt, return_tensors=\"pt\", truncation=False, padding=\"max_length\",\n",
        "                                        max_length=shape_max_length).input_ids.to(self.__device_name)\n",
        "\n",
        "        concat_embeds = []\n",
        "        neg_embeds = []\n",
        "        for i in range(0, shape_max_length, max_length):\n",
        "            concat_embeds.append(self._pipe.text_encoder(input_ids[:, i: i + max_length])[0])\n",
        "            neg_embeds.append(self._pipe.text_encoder(negative_ids[:, i: i + max_length])[0])\n",
        "\n",
        "        return torch.cat(concat_embeds, dim=1), torch.cat(neg_embeds, dim=1)\n",
        "\n",
        "    def __extract_filename(self, full_path):\n",
        "        filename = os.path.basename(full_path)\n",
        "        return os.path.splitext(filename)[0]\n",
        "\n",
        "    def __read_file_list(self, directory, extension):\n",
        "        file_list = []\n",
        "        for filename in os.listdir(directory):\n",
        "            for ext in extension.split('|'):\n",
        "                if filename.endswith(ext):\n",
        "                    file_list.append(filename)\n",
        "        return file_list\n",
        "\n",
        "    def reload_embeddings(self):\n",
        "        print('reload_embeddings')\n",
        "        if self._pipe is None:\n",
        "            print('no model loaded!')\n",
        "            return\n",
        "\n",
        "        current_file_list = self.__read_file_list(modelpaths.embeddings, '.pt|.bin|.safetensors')\n",
        "        for embeddingsfile in current_file_list:\n",
        "            if embeddingsfile not in self.__loaded_embeddings:\n",
        "                try:\n",
        "                    self._pipe.load_textual_inversion(modelpaths.embeddings + '/' + embeddingsfile, token=self.__extract_filename(embeddingsfile))\n",
        "                    self.__loaded_embeddings.append(embeddingsfile)\n",
        "                except Exception as e:\n",
        "                    print(f'error loading {embeddingsfile}')\n",
        "\n",
        "        print('finish reload_embeddings: total loaded = ' + str(len(self.__loaded_embeddings)))\n",
        "\n",
        "    def runText2Img(self, prompt, negative_prompt, step=20, gs=6, w=512, h=512, batch=1):\n",
        "        if self._pipe is None:\n",
        "            print('no model loaded!')\n",
        "            return\n",
        "\n",
        "        # p, np = self.__get_pipeline_embeds(prompt, negative_prompt)\n",
        "        p, np = text_embeddings(self._pipe, prompt, negative_prompt, clip_stop_at_last_layers=self._clip_skip)\n",
        "\n",
        "        image = self._pipe(prompt_embeds=p, negative_prompt_embeds=np, generator=torch.manual_seed(self.seed),\n",
        "            height=h, width=w, guidance_scale=gs, num_inference_steps=step, num_images_per_prompt=batch).images\n",
        "\n",
        "        return image\n",
        "        # for i in range(batch):\n",
        "        #     image[i].save(f\"./image{i+1}.png\")\n",
        "\n",
        "    def runImg2Img(self, prompt, negative_prompt, image, strength=0.35, step=20, gs=6, batch=1):\n",
        "        if self._pipe is None:\n",
        "            print('no model loaded!')\n",
        "            return\n",
        "\n",
        "        components = self._pipe.components\n",
        "        img2img_pipe = StableDiffusionImg2ImgPipeline(**components)\n",
        "\n",
        "        # p, np = self.__get_pipeline_embeds(prompt, negative_prompt)\n",
        "        p, np = text_embeddings(img2img_pipe, prompt, negative_prompt, clip_stop_at_last_layers=self._clip_skip)\n",
        "\n",
        "        images = img2img_pipe(prompt_embeds=p, negative_prompt_embeds=np, generator=torch.manual_seed(self.seed),\n",
        "            strength=strength, image=image, guidance_scale=gs, num_inference_steps=step, num_images_per_prompt=batch).images\n",
        "\n",
        "        img2img_pipe = None\n",
        "        return images\n",
        "\n",
        "    def apply_lora(self, filename, alpha=1.0):\n",
        "        if self._pipe is None:\n",
        "            print('no model loaded!')\n",
        "            return\n",
        "        lora = modelpaths.lora + '/' + filename\n",
        "        if not os.path.exists(lora):\n",
        "            print(f\"{filename} not found !\")\n",
        "            return\n",
        "\n",
        "        change = False\n",
        "        found = False\n",
        "        lora_file_name = filename.split('.')[0]\n",
        "        for i in range(len(self.__loaded_lora)):\n",
        "            la = self.__loaded_lora[i]\n",
        "            if la[0] == lora_file_name:\n",
        "                found = True\n",
        "                if la[0] != alpha:\n",
        "                    self.__loaded_lora[i][1] = alpha\n",
        "                    change = True\n",
        "                break\n",
        "\n",
        "        if not found:\n",
        "            self._pipe.load_lora_weights(\".\", weight_name=lora, adapter_name=lora_file_name)\n",
        "            self.__loaded_lora.append([lora_file_name, alpha])\n",
        "            change = True\n",
        "\n",
        "        if change:\n",
        "            lora_list = []\n",
        "            lora_weights_list = []\n",
        "            for i in range(len(self.__loaded_lora)):\n",
        "                la = self.__loaded_lora[i]\n",
        "                if la[1] != 0:\n",
        "                    lora_list.append(la[0])\n",
        "                    lora_weights_list.append(la[1])\n",
        "            self._pipe.set_adapters(lora_list, adapter_weights=lora_weights_list)\n",
        "\n",
        "    def remove_lora(self, filename):\n",
        "        # if self._pipe is None:\n",
        "        #     print('no model loaded!')\n",
        "        #     return\n",
        "        # lora_file_name = filename.split('.')[0]\n",
        "\n",
        "        # for i in range(len(self.__loaded_lora)):\n",
        "        #     la = self.__loaded_lora[i]\n",
        "        #     if la[0] == lora_file_name:\n",
        "        #         self.__loaded_lora.remove(la)\n",
        "        self.apply_lora(filename, 0)\n",
        "\n",
        "    def scaleImageBy(self, images, scale_factor, type='bicubic'):\n",
        "        image_tensor = self._upscaler_model.convert_PIL_Image_to_torch_tensor(images)\n",
        "        im = self._upscaler_model.upscale(image_tensor, type, scale_factor)\n",
        "        return self._upscaler_model.convert_torch_tensor_to_PIL_Image(im)\n",
        "\n",
        "    def scaleImageByModel(self, images, model : UpscalerModel = UpscalerModel.RealESRGAN_x2):\n",
        "        esrgan_model = None\n",
        "        if model == SDLoader.UpscalerModel.RealESRGAN_x2:\n",
        "            if self._upscaler_type != SDLoader.UpscalerModel.RealESRGAN_x2:\n",
        "                if not os.path.exists(modelpaths.upscale + '/' + SDLoader.UpscalerModel.RealESRGAN_x2.value[1]):\n",
        "                    download(SDLoader.UpscalerModel.RealESRGAN_x2.value[0], SDLoader.UpscalerModel.RealESRGAN_x2.value[1], modelpaths.upscale)\n",
        "                    if not os.path.exists(modelpaths.upscale + '/' + SDLoader.UpscalerModel.RealESRGAN_x2.value[1]):\n",
        "                        raise Exception('SDLoader.scaleImageByModel: download RealESRGAN_x2 failed!')\n",
        "\n",
        "                _upscaler_type=SDLoader.UpscalerModel.RealESRGAN_x2\n",
        "                self._esrgan_model = self._upscaler_model.load_model(modelpaths.upscale + '/' + SDLoader.UpscalerModel.RealESRGAN_x2.value[1])\n",
        "\n",
        "        elif model == SDLoader.UpscalerModel.UltraSharp_4x:\n",
        "            if self._upscaler_type != SDLoader.UpscalerModel.UltraSharp_4x:\n",
        "                if not os.path.exists(modelpaths.upscale + '/' + SDLoader.UpscalerModel.UltraSharp_4x.value[1]):\n",
        "                    download(SDLoader.UpscalerModel.UltraSharp_4x.value[0], SDLoader.UpscalerModel.UltraSharp_4x.value[1], modelpaths.upscale)\n",
        "                    if not os.path.exists(modelpaths.upscale + '/' + SDLoader.UpscalerModel.UltraSharp_4x.value[1]):\n",
        "                        raise Exception('SDLoader.scaleImageByModel: download 4x-UltraSharp failed!')\n",
        "\n",
        "                _upscaler_type=SDLoader.UpscalerModel.UltraSharp_4x\n",
        "                self._esrgan_model = self._upscaler_model.load_model(modelpaths.upscale + '/' + SDLoader.UpscalerModel.UltraSharp_4x.value[1])\n",
        "        else:\n",
        "            raise Exception('SDLoader.scaleImageByModel: model not supported!')\n",
        "\n",
        "        image_tensor = self._upscaler_model.convert_PIL_Image_to_torch_tensor(images)\n",
        "        im = self._upscaler_model.upscale_by_model(self._esrgan_model, image_tensor)\n",
        "        return self._upscaler_model.convert_torch_tensor_to_PIL_Image(im)\n",
        "\n",
        "    class ControlnetModel(Enum):\n",
        "        Canny = ['https://huggingface.co/lllyasviel/control_v11p_sd15_canny/resolve/main/diffusion_pytorch_model.fp16.safetensors?download=true', 'control_v11p_sd15_canny.fp16.safetensors']\n",
        "        Depth = ['https://huggingface.co/lllyasviel/control_v11f1p_sd15_depth/resolve/main/diffusion_pytorch_model.fp16.safetensors?download=true', 'control_v11f1p_sd15_depth.fp16.safetensors']\n",
        "        SoftEdge = ['https://huggingface.co/lllyasviel/control_v11p_sd15_softedge/resolve/main/diffusion_pytorch_model.fp16.safetensors?download=true', 'control_v11p_sd15_softedge.fp16.safetensors']\n",
        "        Inpaint = ['https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint/resolve/main/diffusion_pytorch_model.fp16.safetensors?download=true', 'control_v11p_sd15_inpaint.fp16.safetensors']\n",
        "        OpenPose = ['https://huggingface.co/lllyasviel/control_v11p_sd15_openpose/resolve/main/diffusion_pytorch_model.fp16.safetensors?download=true', 'control_v11p_sd15_openpose.fp16.safetensors']\n",
        "        Scribble = ['https://huggingface.co/lllyasviel/control_v11p_sd15_scribble/resolve/main/diffusion_pytorch_model.fp16.safetensors?download=true', 'control_v11p_sd15_scribble.fp16.safetensors']\n",
        "        LineArt  = ['https://huggingface.co/lllyasviel/control_v11p_sd15_lineart/resolve/main/diffusion_pytorch_model.fp16.safetensors?download=true', 'control_v11p_sd15_lineart.fp16.safetensors']\n",
        "\n",
        "    def applyControlnet(self, model : ControlnetModel, src_image, strength=1):\n",
        "        if self._pipe is None:\n",
        "            print('no model loaded!')\n",
        "            return\n",
        "        cnet_path = modelpaths.controlnet + '/' + model.value[1]\n",
        "        download(model.value[0], model.value[1], modelpaths.controlnet)\n",
        "        if not os.path.exists(cnet_path):\n",
        "            raise Exception(f'SDLoader.applyControlnet: download {model.value.name} failed!')\n",
        "\n",
        "        TODO('')\n",
        "        # controlnet = ControlNetModel.from_single_file(url)\n",
        "        # pipe = StableDiffusionControlNetPipeline.from_single_file(url, controlnet=controlnet)\n",
        "        # https://github.com/huggingface/diffusers/pull/2407#issuecomment-1451951059\n",
        "        # pipe.controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-normal\", torch_dtype=torch.float16)\n",
        "\n",
        "    def getPip(self):\n",
        "        return self._pipe\n",
        "\n",
        "    def applyIPAdapter(self, image):\n",
        "        self._pipe.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter-full-face_sd15.bin\")\n",
        "        self._pipe.ip_adapter_image=image\n",
        "\n",
        "sdloader = SDLoader()\n",
        "sdloader.loadModel('Realistic_Vision_V5.0.safetensors')"
      ],
      "metadata": {
        "id": "BhZ9IzzP-ziN",
        "outputId": "f1c0a697-e36d-4ff9-cf68-6d535afd4f95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "/usr/local/lib/python3.10/dist-packages/diffusers/pipelines/pipeline_utils.py:761: FutureWarning: `torch_dtype` is deprecated and will be removed in version 0.25.0. \n",
            "  deprecate(\"torch_dtype\", \"0.25.0\", \"\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reload_embeddings\n",
            "error loading nrealfixer.pt\n",
            "error loading nartfixer.pt\n",
            "error loading bad-image-v2-39000.pt\n",
            "error loading 21charturnerv2.pt\n",
            "error loading nfixer.pt\n",
            "finish reload_embeddings: total loaded = 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def plot_images(images, labels = None):\n",
        "    N = len(images)\n",
        "    n_cols = 3\n",
        "    n_rows = int(np.ceil(N / n_cols))\n",
        "\n",
        "    if images[0].width > images[0].height:\n",
        "        plt.figure(figsize = (7 * n_cols, 5 * n_rows))\n",
        "    else:\n",
        "        plt.figure(figsize = (5 * n_cols, 7 * n_rows))\n",
        "    for i in range(len(images)):\n",
        "        plt.subplot(n_rows, n_cols, i + 1)\n",
        "        if labels is not None:\n",
        "            plt.title(labels[i])\n",
        "        plt.imshow(np.array(images[i]))\n",
        "        plt.axis(False)\n",
        "    plt.show()\n",
        "\n",
        "prompt = \"\"\"realistic fantastic fullbody 1girl warriors, king, highly detailed liny black gold suit,\n",
        "        fantasy war background, highly detailed eyes, highly detailed realistic face skin, enhanced, 8k\"\"\"\n",
        "\n",
        "negative_prompt=\"(worst quality, low quality:1.4), FastNegativeEmbedding, bad-hands-5\"\n",
        "sdloader.seed=26553312\n",
        "# sdloader.change_scheduler(SDLoader.Scheduler.Euler)\n",
        "# sdloader.setClipSkip(1)\n",
        "# sdloader.apply_lora('blackgold-000008.safetensors', 0.2)\n",
        "# sdloader.apply_lora('more_details.safetensors', 0.8)\n",
        "images = sdloader.runText2Img(prompt, negative_prompt, 20, 8, 512, 768, 3)\n",
        "# sdloader.remove_lora(c)\n",
        "plot_images(images, range(len(images)))"
      ],
      "metadata": {
        "id": "utSt5TtL_Abz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "limg = Image.open('/content/photo_2023-11-28_20-31-07.jpg')\n",
        "sdloader.applyIPAdapter(limg)\n",
        "images = sdloader.runText2Img(prompt, negative_prompt, 20, 8, 512, 768, 3)\n",
        "plot_images(images, range(len(images)))"
      ],
      "metadata": {
        "id": "O3clbqafjz8J",
        "outputId": "c7b9aba6-ab10-4648-9fc2-8b4533d9c0c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-c384411c15bc>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/photo_2023-11-28_20-31-07.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msdloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplyIPAdapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msdloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunText2Img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-9e95b21c791d>\u001b[0m in \u001b[0;36mapplyIPAdapter\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplyIPAdapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_ip_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"h94/IP-Adapter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"models\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ip-adapter-full-face_sd15.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mip_adapter_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/loaders/ip_adapter.py\u001b[0m in \u001b[0;36mload_ip_adapter\u001b[0;34m(self, pretrained_model_name_or_path_or_dict, subfolder, weight_name, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# load ip-adapter into unet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_ip_adapter_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_ip_adapter_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/loaders/unet.py\u001b[0m in \u001b[0;36m_load_ip_adapter_weights\u001b[0;34m(self, state_dict)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0;31m# create image projection layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m         \u001b[0mclip_embeddings_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_proj\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"proj.weight\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mcross_attention_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_proj\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"proj.weight\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'proj.weight'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sr_images = sdloader.scaleImageByModel(images, SDLoader.UpscalerModel.UltraSharp_4x)\n",
        "sr_images = sdloader.scaleImageBy(sr_images, 0.7, type='bicubic')\n"
      ],
      "metadata": {
        "id": "yuZM6fJU3aDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2 = prompt + ''\n",
        "images2 = sdloader.runImg2Img(prompt2, negative_prompt, sr_images[2], strength=0.4, step=40, gs=8, batch=1)\n",
        "saveJPEG(images2[0], '/content/out', 'wm3')\n",
        "plot_images(images2, range(len(images2)))"
      ],
      "metadata": {
        "id": "nlnZ8GBTTO7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdloader.unload()\n",
        "sdloader = None"
      ],
      "metadata": {
        "id": "jGSCu_BtpIUu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3o2V6urxXMe_"
      }
    }
  ]
}